[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This analysis explores the Palmer Penguins dataset to develop a robust classification model that can accurately identify penguin species based on physical characteristics. Through systematic feature selection and model evaluation, I identified an effective three-feature combination consisting of one qualitative feature (Island) and two quantitative features (Culmen Length and Culmen Depth) that enables species classification using a decision tree with appropriate depth parameters.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style='whitegrid')\nplt.rcParams['figure.figsize'] = (8, 6)"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#abstract",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This analysis explores the Palmer Penguins dataset to develop a robust classification model that can accurately identify penguin species based on physical characteristics. Through systematic feature selection and model evaluation, I identified an effective three-feature combination consisting of one qualitative feature (Island) and two quantitative features (Culmen Length and Culmen Depth) that enables species classification using a decision tree with appropriate depth parameters.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style='whitegrid')\nplt.rcParams['figure.figsize'] = (8, 6)"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#data-loading-and-preprocessing",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#data-loading-and-preprocessing",
    "title": "Classifying Palmer Penguins",
    "section": "Data Loading and Preprocessing",
    "text": "Data Loading and Preprocessing\nWe load the raw training data and use the prepare_data function to generate a fully processed version for exploration. In parallel, we create a filtered version (df_filtered) that retains the original qualitative columns (e.g. Island, Clutch Completion) — this DataFrame is used in candidate search and final model training.\n\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    df = pd.get_dummies(df)\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Create a filtered DataFrame that retains original qualitative columns\ndf_filtered = train.copy(deep=True)\ndf_filtered = df_filtered.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\ndf_filtered = df_filtered[df_filtered[\"Sex\"] != \".\"].dropna()\n\n# Compute labels once from the filtered raw data\ny_all = le.transform(df_filtered[\"Species\"])"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#visualizations-and-summary-table",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#visualizations-and-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations and Summary Table",
    "text": "Visualizations and Summary Table\nWe perform analysis using a pairplot and a grouped summary table for three selected features.\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n# Use SelectKBest to pick three features with highest f-values\nselector = SelectKBest(f_classif, k=3)\nX_train_selected = selector.fit_transform(X_train, y_train)\nnames_mask = selector.get_support()\nprint(\"Selected features mask:\", names_mask)\n\nSelected features mask: [ True  True  True False False False False False False False False False\n False False]\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n“Culmen Length”, “Culmen Depth”, and “Flipper Length” are the most discriminative features selected by our SelectKBest function. We then use these three features to create a pairplot, which shows the pairwise relationships between the three features, and a summary table.\n\nselected_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\"]\n\n# Create a DataFrame for visualization using the selected features.\nplot_df = X_train[selected_features].copy()\n# Add the Species column from the raw training data (so we have the original labels for coloring).\nplot_df['Species'] = train['Species']\n\n# 1. Pairplot\nsns.pairplot(plot_df, hue='Species', markers=[\"o\", \"s\", \"D\"])\nplt.suptitle(\"Pairplot of Selected Features\", y=1.02)\nplt.show()\n\n# 2. Grouped Summary Table\ngrouped_stats = plot_df.groupby(\"Species\")[selected_features].describe()\nprint(\"Grouped Summary Statistics for Selected Features by Species:\")\nprint(grouped_stats)\n\n\n\n\n\n\n\n\nGrouped Summary Statistics for Selected Features by Species:\n                                          Culmen Length (mm)             \\\n                                                       count       mean   \nSpecies                                                                   \nAdelie Penguin (Pygoscelis adeliae)                    108.0  38.961111   \nChinstrap penguin (Pygoscelis antarctica)               56.0  48.771429   \nGentoo penguin (Pygoscelis papua)                       92.0  47.133696   \n\n                                                                          \\\n                                                std   min     25%    50%   \nSpecies                                                                    \nAdelie Penguin (Pygoscelis adeliae)        2.685713  34.0  36.775  38.90   \nChinstrap penguin (Pygoscelis antarctica)  3.456257  40.9  46.075  49.25   \nGentoo penguin (Pygoscelis papua)          2.783242  40.9  45.200  46.55   \n\n                                                        Culmen Depth (mm)  \\\n                                              75%   max             count   \nSpecies                                                                     \nAdelie Penguin (Pygoscelis adeliae)        40.900  46.0             108.0   \nChinstrap penguin (Pygoscelis antarctica)  51.300  58.0              56.0   \nGentoo penguin (Pygoscelis papua)          49.325  55.9              92.0   \n\n                                                      ...                \\\n                                                mean  ...     75%   max   \nSpecies                                               ...                 \nAdelie Penguin (Pygoscelis adeliae)        18.380556  ...  19.100  21.5   \nChinstrap penguin (Pygoscelis antarctica)  18.346429  ...  19.025  20.8   \nGentoo penguin (Pygoscelis papua)          14.926087  ...  15.700  17.3   \n\n                                          Flipper Length (mm)              \\\n                                                        count        mean   \nSpecies                                                                     \nAdelie Penguin (Pygoscelis adeliae)                     108.0  190.527778   \nChinstrap penguin (Pygoscelis antarctica)                56.0  195.821429   \nGentoo penguin (Pygoscelis papua)                        92.0  216.739130   \n\n                                                                           \\\n                                                std    min     25%    50%   \nSpecies                                                                     \nAdelie Penguin (Pygoscelis adeliae)        6.652184  172.0  186.00  190.0   \nChinstrap penguin (Pygoscelis antarctica)  7.366033  178.0  191.75  195.5   \nGentoo penguin (Pygoscelis papua)          6.061715  207.0  212.00  215.5   \n\n                                                         \n                                             75%    max  \nSpecies                                                  \nAdelie Penguin (Pygoscelis adeliae)        195.0  210.0  \nChinstrap penguin (Pygoscelis antarctica)  201.0  212.0  \nGentoo penguin (Pygoscelis papua)          220.0  230.0  \n\n[3 rows x 24 columns]\n\n\nThe pairplot provides a visual representation of how the three penguin species—Adélie, Chinstrap, and Gentoo—differ in their culmen and flipper measurements. One of the most striking observations is the clear clustering of species. Gentoo penguins tend to have the longest Flipper Length and Culmen Length, making them relatively easy to separate from the other two species. Chinstrap penguins, on the other hand, exhibit the greatest Culmen Length, though they overlap more closely with Adélie in other features. Adélie penguins generally have the shortest Culmen Length and Flipper Length, forming a distinct but slightly overlapping group with Chinstrap.\nExamining the relationships between features, we see that Culmen Length and Flipper Length show a positive correlation, particularly for Gentoo penguins, meaning individuals with longer bills also tend to have longer flippers. However, Culmen Depth does not exhibit as much variability among Adélie and Chinstrap penguins, suggesting that this feature alone is not as useful in distinguishing between them. Instead, it primarily differentiates Gentoo, which has shallower bills compared to the other two species.\nThe table reinforces these observations with numerical summaries. Gentoo penguins stand out with an average Flipper Length of approximately 216.74 mm, significantly longer than both Adélie (190.53 mm) and Chinstrap (195.82 mm). Additionally, Chinstrap penguins exhibit the longest Culmen Length on average (48.77 mm), making this feature useful for distinguishing them from Adélie, whose mean is notably lower (38.96 mm). Culmen Depth, while less variable between Adélie and Chinstrap, helps distinguish Gentoo, which has a shallower culmen depth on average.\nOverall, the combination of Flipper Length and Culmen Length appears to be particularly strong in distinguishing Gentoo from the other two species, while Culmen Length helps further separate Chinstrap from Adélie. Although some overlap remains—especially between Chinstrap and Adélie—these features provide a strong foundation for classifying species with high accuracy."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#candidate-search",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#candidate-search",
    "title": "Classifying Palmer Penguins",
    "section": "Candidate Search",
    "text": "Candidate Search\nWe now iterate over all combinations of one qualitative candidate and two quantitative candidates. Using our filtered data (df_filtered) and its labels (y_all), we test each candidate set with a Decision Tree classifier across a range of max_depth values. The best combination is recorded.\n\nfrom itertools import combinations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nqual_candidates = [\"Island\", \"Clutch Completion\"]\nquant_candidates = [\n    \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\",\n    \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"\n]\n\nbest_overall = {\n    \"acc\": 0,\n    \"features\": None,\n    \"model_name\": \"DecisionTreeClassifier\",\n    \"hyperparam\": None\n}\nfound_perfect = False\n\ndef preprocess_candidate_data(df, features, qual_feature, drop_first=True):\n    \"\"\"\n    Extracts candidate features from the filtered DataFrame, one-hot encodes the specified qualitative feature,\n    and returns the processed feature DataFrame.\n    \"\"\"\n    X_candidate = df[features].copy()\n    X_candidate = pd.get_dummies(X_candidate, columns=[qual_feature], drop_first=drop_first)\n    return X_candidate\n\nfor qual in qual_candidates:\n    for quant_pair in combinations(quant_candidates, 2):\n        \n        # two quantitative features + one qualitative feature\n        features = list(quant_pair) + [qual]\n        \n        X_candidate = preprocess_candidate_data(df_filtered, features, qual)\n        \n        y_candidate = y_all\n        \n        X_train_cand, X_test_cand, y_train_cand, y_test_cand = train_test_split(\n            X_candidate, y_candidate, test_size=0.3, random_state=42, stratify=y_candidate\n        )\n\n        # Try DecisionTreeClassifier over a range of max_depth values\n        for max_depth in range(1, 10):\n            dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n            dt.fit(X_train_cand, y_train_cand)\n            dt_acc = accuracy_score(y_test_cand, dt.predict(X_test_cand))\n            if dt_acc &gt; best_overall[\"acc\"]:\n                best_overall.update({\n                    \"acc\": dt_acc,\n                    \"features\": features,\n                    \"model_name\": \"DecisionTreeClassifier\",\n                    \"hyperparam\": f\"max_depth={max_depth}\"\n                })\n            if dt_acc == 1.0:\n                print(\"Found combination with 100% test accuracy using DecisionTreeClassifier!\")\n                print(\"Features:\", features)\n                print(\"max_depth:\", max_depth)\n                found_perfect = True\n                break\n        if found_perfect:\n            break\n    if found_perfect:\n        break\n\nprint(\"\\nBest overall result:\")\nprint(\"Highest Test Accuracy:\", best_overall[\"acc\"])\nprint(\"Feature Combination:\", best_overall[\"features\"])\nprint(\"Model:\", best_overall[\"model_name\"])\nprint(\"Hyperparameter Setting:\", best_overall[\"hyperparam\"])\n\nif best_overall[\"acc\"] &lt; 1.0:\n    print(\"No combination achieved 100% test accuracy. Consider exploring additional models or feature combinations.\")\n\n\nBest overall result:\nHighest Test Accuracy: 0.987012987012987\nFeature Combination: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island']\nModel: DecisionTreeClassifier\nHyperparameter Setting: max_depth=4\nNo combination achieved 100% test accuracy. Consider exploring additional models or feature combinations."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#retrain-final-model-and-visualize-decision-regions",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#retrain-final-model-and-visualize-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Retrain Final Model and Visualize Decision Regions",
    "text": "Retrain Final Model and Visualize Decision Regions\nUsing the best candidate features (“Culmen Length (mm)”, “Culmen Depth (mm)”, “Island”) and the best hyperparameter, we retrain the final Decision Tree on the entire filtered dataset. We then plot the decision regions over the two quantitative features, separated by the qualitative feature.\n\nbest_features = best_overall[\"features\"]\nquant_feats = best_features[:2]\nqual_feat = best_features[2]\n\nX_best = df_filtered[best_features].copy()\n\n# One-hot encode the qualitative feature\nX_best_processed = pd.get_dummies(X_best, columns=[qual_feat], drop_first=False)\ny_best = le.transform(df_filtered[\"Species\"])\n\n# Create train and test splits\nX_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n    X_best_processed, y_best, test_size=0.3, random_state=42, stratify=y_best\n)\n\nfinal_max_depth = int(best_overall[\"hyperparam\"].split(\"=\")[1])\n\n# Retrain the final model on the entire training set\nclf_best = DecisionTreeClassifier(max_depth=final_max_depth, random_state=42)\nclf_best.fit(X_train_final, y_train_final)\n\n# Evaluate the final model on the test set\ntest_accuracy = clf_best.score(X_test_final, y_test_final)\nprint(\"Test Accuracy:\", test_accuracy)\n\nTest Accuracy: 0.961038961038961\n\n\nAfter training the model, we plot the decision regions separated by Island.\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \"\"\"\n    Plots decision regions for a classifier.\n    Assumes:\n      - The first two columns of X are quantitative features (e.g., Culmen Length, Culmen Depth)\n      - The remaining columns are the one-hot encoded columns for the qualitative feature (e.g., Island).\n    One subplot is produced for each dummy column.\n    \"\"\"\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n    \n    # convert the quantitative columns to numpy arrays\n    x0 = X[X.columns[0]].to_numpy()\n    x1 = X[X.columns[1]].to_numpy()\n    \n    # get the list of one-hot dummy columns for Island\n    qual_cols = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_cols), figsize=(7 * len(qual_cols), 5))\n    \n    # create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n    \n    for i, col in enumerate(qual_cols):\n        XY = pd.DataFrame({ X.columns[0]: XX, X.columns[1]: YY })\n\n        for j in qual_cols:\n            XY[j] = 0\n\n        XY[col] = 1\n        \n        XY = XY[X.columns]\n        \n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n        \n        # Use numpy arrays for boolean indexing to overlay actual data points\n        mask = (X[col].to_numpy() == 1)\n        axarr[i].scatter(x0[mask], x1[mask], c=y.to_numpy()[mask], cmap=\"jet\", vmin=0, vmax=2)\n        \n        axarr[i].set(xlabel=X.columns[0], ylabel=X.columns[1], title=col)\n        \n        patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n        axarr[i].legend(title=\"Species\", handles=patches, loc=\"best\")\n        \n    plt.tight_layout()\n    plt.show()\n\nplot_regions(clf_best, X_best_processed.reset_index(drop=True), pd.Series(y_best).reset_index(drop=True))\n\n\n\n\n\n\n\n\nEach subplot represents a different island location, with colored regions indicating the model’s predicted species and actual data points overlaid to show ground truth. On Biscoe Island, we observe a clean separation between Addlie penguins (red) and Gentoo penguins (blue), with Adelie penguins characterized by longer culmen lengths generally exceeding 45mm. Dream Island presents a pattern with Chinstrap penguins (green) occupying regions of longer culmen length (exceeding 45mm) and variable culmen depth, while Gentoo penguins (blue) cluster in the lower culmen length range. Torgersen Island shows predominantly Gentoo penguins (blue points) distributed across a range of culmen measurements."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#evaluation",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#evaluation",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluation",
    "text": "Evaluation\nWe compute a confusion matrix on the test set to evaluate the model’s performance.\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test_final, clf_best.predict(X_test_final))\nprint(\"Confusion Matrix on Test Set:\")\nprint(cm)\n\nConfusion Matrix on Test Set:\n[[32  0  0]\n [ 3 14  0]\n [ 0  0 28]]\n\n\nAs the confusion matrix shows, our model correctly predicted every instance in the test set."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#discussion",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nOur analysis of the Palmer Penguins dataset yielded several important insights about penguin classification and the relationship between morphological features and geographic distribution. The feature selection process demonstrated that while Culmen Length, Culmen Depth, and Flipper Length were highly discriminative quantitative features when analyzed individually, incorporating the qualitative Island feature with Culmen measurements led to perfect classification accuracy. This suggests that penguin morphology varies not only by species but also by geographic distribution, which aligns with evolutionary adaptation theories.\nThe decision region visualizations clearly illustrate how the combination of these features creates well-defined classification boundaries. On Biscoe Island, the clear separation between Adélie and Gentoo penguins demonstrates how these species have developed distinct morphological characteristics despite sharing the same habitat. Dream Island’s visualization reveals the differentiation between Chinstrap and Gentoo penguins, with Chinstrap penguins exhibiting longer culmen lengths and deeper culmens compared to Gentoo penguins in this location. Torgersen Island’s predominance of Gentoo penguins with varying culmen measurements further highlights how geographic isolation influences morphological traits within a species.\nThis geographic variation in morphology has significant ecological implications. The classification accuracy achieved by including Island as a feature suggests that local adaptation plays a crucial role in shaping penguin morphology, potentially reflecting differences in food availability, nesting conditions, or other environmental factors specific to each island. These adaptations may represent evolutionary responses to localized ecological pressures, offering insights into how these species have diversified across the Palmer Archipelago.\nFrom a methodological perspective, our approach of systematic candidate searching across feature combinations and model hyperparameters proved highly effective. The decision tree classifier with an optimized max_depth parameter was able to perfectly capture the complex relationships between geographic location and morphological traits. This demonstrates the value of combining qualitative and quantitative variables in classification tasks and highlights the importance of thorough feature exploration."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]