[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html",
    "href": "posts/Replication_Study/Replication_Study.html",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "This analysis explores the racial bias present in a healthcare algorithm that disproportionately affected Black patients by underestimating their healthcare needs. By reproducing key visualizations from Obermeyer et al.’s groundbreaking 2019 paper, I confirm that at equivalent risk scores, Black patients exhibit significantly more chronic conditions than White patients, yet they incur lower healthcare costs. Using polynomial regression modeling on standardized data, I find that Black patients generate approximately 18.9% less in healthcare costs than White patients with equivalent health needs. These findings suggest that using healthcare costs as a proxy for health needs fundamentally encodes racial disparities into the algorithm. This case exemplifies calibration bias in classification algorithms, where equal predictions across groups mask significant disparities in outcomes."
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#abstract",
    "href": "posts/Replication_Study/Replication_Study.html#abstract",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "This analysis explores the racial bias present in a healthcare algorithm that disproportionately affected Black patients by underestimating their healthcare needs. By reproducing key visualizations from Obermeyer et al.’s groundbreaking 2019 paper, I confirm that at equivalent risk scores, Black patients exhibit significantly more chronic conditions than White patients, yet they incur lower healthcare costs. Using polynomial regression modeling on standardized data, I find that Black patients generate approximately 18.9% less in healthcare costs than White patients with equivalent health needs. These findings suggest that using healthcare costs as a proxy for health needs fundamentally encodes racial disparities into the algorithm. This case exemplifies calibration bias in classification algorithms, where equal predictions across groups mask significant disparities in outcomes."
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#panel-a-expenditures-by-risk-score-percentile",
    "href": "posts/Replication_Study/Replication_Study.html#panel-a-expenditures-by-risk-score-percentile",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "1. Panel A: Expenditures by Risk Score Percentile",
    "text": "1. Panel A: Expenditures by Risk Score Percentile\nFirst, we divide the risk scores into 100 quantile bins. By using pd.qcut(), we ensure an approximately equal number of patients in each bin, which helps create a more balanced visualization. The duplicates='drop' parameter ensures we handle any tied values properly. Next, we aggregate the data by both risk percentile bin and race, calculating the mean risk percentile and mean expenditures for each group. This summarization allows us to visualize how costs vary across different risk levels for each racial group. We also count the unique bins to understand how many distinct groups we’ve created.\n\n# Create 100 bins based on the raw risk scores using qcut\ndf['risk_percentile_bin'] = pd.qcut(df['risk_score_t'], q=100, labels=False, duplicates='drop')\n\n# Group by these bins and race to compute the mean risk percentile and mean expenditures\npanelA = df.groupby(['risk_percentile_bin', 'race']).agg(\n    mean_risk_percentile=('risk_percentile', 'mean'),\n    mean_expenditures=('cost_t', 'mean')\n).reset_index()\n\n# Check how many unique bins actually got created\nnum_bins_A = df['risk_percentile_bin'].nunique()"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#panel-b-expenditures-by-chronic-conditions",
    "href": "posts/Replication_Study/Replication_Study.html#panel-b-expenditures-by-chronic-conditions",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "2. Panel B: Expenditures by Chronic Conditions",
    "text": "2. Panel B: Expenditures by Chronic Conditions\nFor the second panel, we repeat a similar process but focusing on chronic conditions rather than risk scores. We create 100 quantile bins based on the number of chronic conditions (gagne_sum_t), then aggregate the data to calculate mean chronic conditions and mean expenditures for each bin and race. This allows us to compare how healthcare costs vary with illness burden across racial groups.\n\ndf['chronic_bin'] = pd.qcut(df['gagne_sum_t'], q=100, labels=False, duplicates='drop')\n\n# Group by these bins and race to compute the mean number of chronic conditions and mean expenditures\npanelB = df.groupby(['chronic_bin', 'race']).agg(\n    mean_chronic=('gagne_sum_t', 'mean'),\n    mean_expenditures=('cost_t', 'mean')\n).reset_index()\n\n# Check how many unique bins were created for chronic conditions\nnum_bins_B = df['chronic_bin'].nunique()"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#plotting-both-panels-side-by-side",
    "href": "posts/Replication_Study/Replication_Study.html#plotting-both-panels-side-by-side",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "3. Plotting Both Panels Side by Side",
    "text": "3. Plotting Both Panels Side by Side\nWith our aggregated data, we now create a two-panel figure. For Panel A, we visualize the relationship between risk scores and healthcare expenditures. We use distinctive markers and colors to differentiate between racial groups. By inverting the x-axis, higher risk percentiles appear on the left, matching the original paper’s presentation and making it easier to focus on the highest-risk patients.\nFor Panel B, we construct the second visualization showing the relationship between chronic conditions and healthcare expenditures. Using the same styling approach as Panel A ensures visual consistency. The tight_layout() function optimizes the spacing between subplots for a clean presentation.\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Panel A: Mean Expenditures vs. Mean Risk Percentile\nsns.scatterplot(\n    data=panelA,\n    x='mean_risk_percentile',\n    y='mean_expenditures',\n    hue='race',\n    style='race',\n    markers={'white': 'X', 'black': 'o'},\n    palette={'white': 'green', 'black': 'blue'},\n    s=80,\n    ax=axes[0]\n)\naxes[0].set_xlabel(\"Mean Algorithm-Predicted Risk Score Percentile\")\naxes[0].set_ylabel(\"Mean Total Medical Expenditures ($)\")\naxes[0].set_title(\"Panel A: Expenditures by Risk Score Percentile (100 Bins)\")\naxes[0].invert_xaxis()  # Reverse x-axis so that 100% is on the left\n\n# Panel B: Mean Expenditures vs. Mean Chronic Conditions\nsns.scatterplot(\n    data=panelB,\n    x='mean_chronic',\n    y='mean_expenditures',\n    hue='race',\n    style='race',\n    markers={'white': 'X', 'black': 'o'},\n    palette={'white': 'green', 'black': 'blue'},\n    s=80,\n    ax=axes[1]\n)\naxes[1].set_xlabel(\"Mean Number of Chronic Conditions\")\naxes[1].set_ylabel(\"Mean Total Medical Expenditures ($)\")\naxes[1].set_title(\"Panel B: Expenditures by Chronic Conditions (100 Bins)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3 reveals two critical aspects of the racial bias in this healthcare algorithm: Panel A shows that the algorithm’s risk scores are well-calibrated across races with respect to costs. At any given risk score percentile, both Black and White patients generate roughly the same healthcare expenditures. From a purely cost-prediction standpoint, the algorithm appears unbiased. Panel B, however, tells a different story: at the same number of chronic conditions, Black patients consistently incur lower healthcare costs than White patients. This disparity is especially pronounced for patients with fewer chronic conditions (which represent the majority of patients in the dataset). This finding illuminates the key problem: the algorithm uses cost as a proxy for health needs, but Black patients generate lower healthcare costs than White patients with the same health needs. The reasons for this disparity could include barriers to accessing care, mistrust of the healthcare system due to historical mistreatment, different treatment patterns by healthcare providers, and socioeconomic factors affecting healthcare utilization."
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#loading-and-preparing-the-data",
    "href": "posts/Replication_Study/Replication_Study.html#loading-and-preparing-the-data",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "1. Loading and Preparing the Data",
    "text": "1. Loading and Preparing the Data\nWe begin by importing essential statistical and machine learning libraries that will enable us to build and evaluate our regression models. Since our analysis involves logarithmic transformation, we filter out patients with zero cost records (as log(0) is mathematically undefined). We then prepare our data by creating a log-transformed cost variable and a binary indicator for race.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score, KFold\nimport warnings\n\n# Keep only white or black patients\ndf = df[df['race'].isin(['white', 'black'])]\n\n# Remove patients with zero cost (log(0) is undefined)\ndf_model = df[df['cost_t'] &gt; 0].copy()\n\n# Create a log-transformed cost column\ndf_model['log_cost'] = np.log(df_model['cost_t'])\n\n# Race dummy: 1 if Black, 0 if White\ndf_model['race_dummy'] = (df_model['race'] == 'black').astype(int)"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#defining-helper-functions",
    "href": "posts/Replication_Study/Replication_Study.html#defining-helper-functions",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "2. Defining Helper Functions",
    "text": "2. Defining Helper Functions\nTo properly model the relationship between health status and costs, we need to account for potential nonlinear patterns. We define two utility functions: one that standardizes variables to improve model convergence, and another that generates polynomial features to capture nonlinear relationships between chronic conditions and healthcare costs.\n\ndef standard_scale(series):\n    return (series - series.mean()) / series.std()\n\ndef add_polynomial_features(df, degree):\n    df_poly = df.copy()\n    for j in range(2, degree + 1):\n        df_poly[f\"poly_{j}\"] = df_poly[\"gagne_sum_t\"] ** j\n    return df_poly"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#building-the-design-matrix",
    "href": "posts/Replication_Study/Replication_Study.html#building-the-design-matrix",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "3. Building the Design Matrix",
    "text": "3. Building the Design Matrix\nThis function constructs our model’s input data by creating a feature matrix that captures both the health status of patients (through polynomial transformations of chronic condition counts) and their racial identity. The standardization step ensures our model doesn’t favor higher-magnitude features, while the polynomial expansion allows us to detect complex patterns in how health status relates to costs.\n\ndef build_design_matrix(df, degree):\n    df_temp = df.copy()\n    \n    # Standardize the predictor\n    df_temp['gagne_sum_t_scaled'] = standard_scale(df_temp['gagne_sum_t'])\n    \n    # Rename the scaled column to 'gagne_sum_t' for polynomial generation\n    df_poly_input = df_temp[['gagne_sum_t_scaled']].rename(columns={'gagne_sum_t_scaled': 'gagne_sum_t'})\n    df_poly = add_polynomial_features(df_poly_input, degree)\n    \n    # Gather polynomial columns into a NumPy array\n    poly_cols = ['gagne_sum_t'] + [f\"poly_{j}\" for j in range(2, degree + 1)]\n    X_poly = df_poly[poly_cols].values\n    \n    # Append the race dummy as an extra column\n    race_dummy = df_temp[['race_dummy']].values\n    X = np.hstack([X_poly, race_dummy])\n    \n    # The target is log_cost\n    y = df_temp['log_cost'].values\n    \n    return X, y"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#cross-validation-over-degree-and-alpha",
    "href": "posts/Replication_Study/Replication_Study.html#cross-validation-over-degree-and-alpha",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "4. Cross-Validation over Degree and Alpha",
    "text": "4. Cross-Validation over Degree and Alpha\nTo find the optimal model that balances complexity against generalizability, we perform a grid search across different polynomial degrees and regularization strengths. We employ 5-fold cross-validation to ensure our model will perform well on new data, and we track the mean squared error to identify the combination that best captures the relationship between health status, race, and healthcare costs.\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n\n    # Define the search space\n    degrees = range(1, 12)\n    alphas = [10**k for k in range(-4, 5)]\n\n    best_mse = np.inf\n    best_degree = None\n    best_alpha = None\n\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n    # Search for the best degree and alpha\n    for deg in degrees:\n        X_all, y_all = build_design_matrix(df_model, deg)\n        \n        for alpha in alphas:\n            ridge = Ridge(alpha=alpha)\n            \n            # Negative MSE is returned, so we negate it to get the actual MSE\n            scores = cross_val_score(ridge, X_all, y_all, scoring='neg_mean_squared_error', cv=cv)\n            mse = -np.mean(scores)\n            \n            if mse &lt; best_mse:\n                best_mse = mse\n                best_degree = deg\n                best_alpha = alpha\n    \n    print(f\"Best polynomial degree: {best_degree}\")\n    print(f\"Best alpha: {best_alpha}\")\n    print(f\"Best CV MSE: {best_mse:.4f}\")\n\nBest polynomial degree: 11\nBest alpha: 0.1\nBest CV MSE: 1.5085"
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#refit-the-final-model-and-print-results",
    "href": "posts/Replication_Study/Replication_Study.html#refit-the-final-model-and-print-results",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "5. Refit the Final Model and Print Results",
    "text": "5. Refit the Final Model and Print Results\nUsing the optimal parameters identified through cross-validation, we now train our final model on the complete dataset. The coefficient for our race dummy variable directly quantifies the disparity in healthcare costs between Black and White patients after controlling for health status. Since our model uses log-transformed costs, we exponentiate and convert to a percentage to provide an intuitive interpretation of the racial disparity in healthcare spending.\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    # Refit using the best combination\n    X_final, y_final = build_design_matrix(df_model, best_degree)\n    final_ridge = Ridge(alpha=best_alpha)\n    final_ridge.fit(X_final, y_final)\n\n    # Extract the coefficient for race_dummy (last column in X_final)\n    race_coef = final_ridge.coef_[-1]\n    print(\"Coefficient for race_dummy (Black vs. White):\", race_coef)\n\n    # Interpretation in a log-cost model\n    perc_diff = (np.exp(race_coef) - 1) * 100\n    print(f\"Estimated % difference in cost for Black vs. White patients: {perc_diff:.2f}%\")\n\nCoefficient for race_dummy (Black vs. White): -0.2668415054849388\nEstimated % difference in cost for Black vs. White patients: -23.42%\n\n\nAfter determining the optimal polynomial degree and regularization strength through cross-validation, we built a final model that reveals that Black patients incur approximately 23.42% less in healthcare costs compared to White patients with the same number and pattern of chronic conditions. This finding strongly supports Obermeyer et al.’s argument that there is a systematic racial disparity in healthcare costs that is not explained by differences in health status. The algorithm, by using cost as a proxy for health needs, inherits this disparity and erroneously concludes that Black patients are healthier than equally sick White patients."
  },
  {
    "objectID": "posts/Replication_Study/Replication_Study.html#part-e-discussion",
    "href": "posts/Replication_Study/Replication_Study.html#part-e-discussion",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part E: Discussion",
    "text": "Part E: Discussion\nThis analysis confirms the central finding of Obermeyer et al.’s 2019 paper: a widely used healthcare algorithm exhibits significant racial bias by underestimating the health needs of Black patients. By predicting future healthcare costs rather than actual illness, the algorithm systematically disadvantages Black patients who face barriers to accessing healthcare despite having similar or greater health needs compared to White patients. Looking at the formal statistical discrimination criteria from Chapter 3 of Barocas, Hardt, and Narayanan (2023), this algorithm’s bias aligns most closely with calibration bias. Calibration requires that for any score value, the predicted probability matches the actual probability of the outcome across all groups. In this case, the algorithm is well-calibrated for costs (Panel A of Figure 3 shows that at any risk score, Black and White patients have similar healthcare costs), but it fails to calibrate for actual health needs (as shown in Figure 1, where Black patients have more chronic conditions than White patients at the same risk score).\nThis discrepancy represents a form of representational harm, where a seemingly accurate algorithm encodes and reinforces societal inequities. By framing health risk in terms of cost rather than illness, the algorithm reproduces historical patterns of healthcare underutilization by Black patients. This case study illustrates how algorithmic bias can emerge even without explicit discrimination or malicious intent. The bias stemmed not from explicit racial variables in the model, but from the choice of using cost as a proxy for health needs - a choice that seemed reasonable but failed to account for societal inequities in healthcare access and utilization. The solution proposed by the researchers – modifying the algorithm to predict health needs directly rather than costs – demonstrates that addressing algorithmic bias often requires reconsidering fundamental assumptions about our data and models rather than simply removing sensitive attributes or adjusting technical parameters."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In a thought-provoking 2022 speech at Princeton University, Arvind Narayanan asserted that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan 2022, 25). This provocative claim challenges foundational assumptions about how researchers study and address discrimination in algorithmic systems. As data-driven systems increasingly influence critical decisions across society—from hiring to lending to criminal justice—the methods we use to evaluate their fairness have profound consequences. This essay examines Narayanan’s position in conversation with other scholarly perspectives to assess whether quantitative approaches to discrimination truly cause more harm than good.\nBy analyzing Narayanan’s critique alongside key scholarly works including Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Data Feminism (D’Ignazio and Klein 2023), and other contributions to the field, I will evaluate the strengths and limitations of quantitative methods in addressing algorithmic bias. Through discussion of both successful and disappointing case studies, I will argue that while Narayanan’s critique identifies crucial shortcomings in current practice, quantitative methods remain essential tools for fairness work when deployed with appropriate critical awareness and integrated into more holistic frameworks for understanding discrimination."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#introduction",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#introduction",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In a thought-provoking 2022 speech at Princeton University, Arvind Narayanan asserted that “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan 2022, 25). This provocative claim challenges foundational assumptions about how researchers study and address discrimination in algorithmic systems. As data-driven systems increasingly influence critical decisions across society—from hiring to lending to criminal justice—the methods we use to evaluate their fairness have profound consequences. This essay examines Narayanan’s position in conversation with other scholarly perspectives to assess whether quantitative approaches to discrimination truly cause more harm than good.\nBy analyzing Narayanan’s critique alongside key scholarly works including Fairness and Machine Learning (Barocas, Hardt, and Narayanan 2023), Data Feminism (D’Ignazio and Klein 2023), and other contributions to the field, I will evaluate the strengths and limitations of quantitative methods in addressing algorithmic bias. Through discussion of both successful and disappointing case studies, I will argue that while Narayanan’s critique identifies crucial shortcomings in current practice, quantitative methods remain essential tools for fairness work when deployed with appropriate critical awareness and integrated into more holistic frameworks for understanding discrimination."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#narayanans-position",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#narayanans-position",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Narayanan’s Position",
    "text": "Narayanan’s Position\nNarayanan’s critique of quantitative methods centers on how they can obscure rather than reveal discrimination. He outlines seven key limitations in current quantitative approaches to discrimination:\nFirst, the null hypothesis in quantitative research typically assumes no discrimination exists, placing the burden of proof on those claiming discrimination. This framing is “not a logical inevitability… [but] a choice” (Narayanan 2022, 7) that inherently favors the status quo. When researchers assume no discrimination exists until proven otherwise, they create a structural barrier to detecting bias.\nSecond, most quantitative methods rely on snapshot datasets that fail to capture how discrimination compounds over time. Using a mathematical model, Narayanan demonstrates how “a 2.5% difference in quarterly performance reviews” can compound over 20 years to create “a 7-fold difference” in CEO demographics (Narayanan 2022, 9–10). Such subtle discrimination falls “far below the threshold that’s detectable by quantitative methods” in a single timeframe.\nThird, data about minoritized groups are often collected by the very institutions suspected of discrimination. This creates conflicts of interest where organizations control what data are collected and released, potentially hiding their own biases.\nFourth, quantitative research tends to advance by “explaining away discrimination” (Narayanan 2022, 12). Academic incentives reward researchers who find omitted variables that can account for disparities, effectively controlling for the attributes that constitute discrimination itself.\nFifth, quantitative approaches often identify the wrong locus of intervention, suggesting that minoritized groups—rather than discriminatory systems—need fixing. Narayanan examines a study of gender pay gaps among Uber drivers that framed the 7% earnings gap as stemming from women’s choices rather than systemic factors, while ignoring that female drivers were “2.7 times as likely to drop off the platform” (Narayanan 2022, 13–14).\nSixth, researchers cling to an objectivity illusion, despite making “at least 10-20 subjective choices” in a typical paper (Narayanan 2022, 16). This illusion of neutrality masks how value judgments shape research design, variable selection, and interpretation.\nFinally, through performativity, narrow statistical definitions of discrimination become operationalized as the only recognized forms of harm. When these metrics become the basis for policy, they limit what counts as actionable discrimination.\nThese limitations lead Narayanan to conclude that quantitative methods often “justify racism and excuse inaction” (Narayanan 2022, 3) by offering technical smokescreens that obscure structural discrimination."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#the-benefits-of-quantitative-methods",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#the-benefits-of-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Benefits of Quantitative Methods",
    "text": "The Benefits of Quantitative Methods\nDespite these limitations, quantitative methods have demonstrated unique value in addressing discrimination when thoughtfully applied. In chapter 3 of Fairness and Machine Learning, Barocas, Hardt, and Narayanan detail how formal statistical criteria can detect and mitigate bias in algorithmic systems. They outline several technical fairness notions, including independence (demographic parity), separation (equal error rates), and sufficiency (calibration by group) (Barocas, Hardt, and Narayanan 2023).\nOne particularly beneficial application referenced by Narayanan himself is Fishbane, Ouss, and Shah’s (2020) study of failure-to-appear rates in the New York City court system. This research exemplifies how quantitative methods can identify concrete intervention points that reduce disparities. In this study, researchers examined why defendants frequently missed court dates for low-level offenses, with particular attention to racial disparities in these rates.\nFrom a technical perspective (Chapter 3), the study employed error rate analysis to measure disparities in court appearance rates across demographic groups. They analyzed patterns in failures to appear, identifying differential impacts on marginalized communities. Their approach relates to what Barocas, Hardt, and Narayanan call separation—ensuring “equal false negative rates” by identifying when different groups experience unequal burdens from the same system (Barocas, Hardt, and Narayanan 2023).\nTheir findings were revelatory: many defendants missed court dates not due to willful evasion but because of confusing summons forms and lack of reminders. The researchers then tested interventions by redesigning the forms and implementing text message reminders, which “drastically reduced the rate at which people failed to appear in court” (Narayanan 2022, 24).\nFrom a moral perspective (Chapter 4), this study embodied what Barocas, Hardt, and Narayanan describe as procedural fairness—ensuring all defendants had equal practical ability to comply with court requirements regardless of their resources or background. The intervention also addressed representational fairness by redesigning forms to be understandable regardless of education level or prior system knowledge (Barocas, Hardt, and Narayanan 2023).\nRather than treating failure to appear as evidence of individual moral failing (which might justify harsher penalties), the researchers recognized it as a system design problem—reflecting the moral principle that individuals should not be penalized for navigating poorly designed bureaucratic processes. This shifts the moral framing from individual culpability to systemic responsibility.\nThis case exemplifies how quantitative methods can productively expose discrimination when they: (1) identify concrete intervention points, (2) focus on system-level rather than individual-level problems, and (3) lead to practical solutions that reduce disparities."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#the-limitations-of-quantitative-methods",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#the-limitations-of-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Limitations of Quantitative Methods",
    "text": "The Limitations of Quantitative Methods\nWhile some quantitative approaches succeed, others reinforce the problems Narayanan identifies. A particularly disappointing study Narayanan highlights is “The Gender Earnings Gap in the Gig Economy: Evidence from over a Million Rideshare Drivers” (Cook et al. 2018), which examined the pay gap between male and female Uber drivers.\nIn technical terms from Chapter 3 of Fairness and Machine Learning, the study focused on disparate impact (the 7% earnings difference) while neglecting the more significant disparate treatment that might explain why female drivers were 2.7 times more likely to leave the platform. The researchers attributed the earnings gap solely to three factors: where drivers chose to drive, men’s greater experience on the platform, and men’s tendency to drive faster.\nThe authors claimed no gender discrimination existed in the platform’s algorithm or rider behavior. But as Narayanan notes, their analysis “explains away discrimination” by treating these factors as neutral preferences rather than potential responses to discriminatory conditions: “part of that is because some neighborhoods aren’t safe for women” and “some women face harassment” (Narayanan 2022, 14).\nFrom a moral perspective (Chapter 4), the study reflects what Barocas, Hardt, and Narayanan call the “difference principle” view of discrimination—treating statistical disparities as morally neutral if they can be explained by apparent preferences or choices. This neglects the broader moral context in which those choices occur. The study treats driver decisions as freely made preferences rather than constrained responses to structural inequities (Barocas, Hardt, and Narayanan 2023).\nThis exemplifies what Selbst et al. (2019) call the “framing trap”—where researchers narrowly define problems in ways that make technical solutions seem sufficient. By focusing on measurable earnings rather than broader experiences of discrimination, the study frames fairness as satisfied by statistical parity alone.\nD’Ignazio and Klein would identify this as a failure to “consider context” and “examine power”—two key principles of Data Feminism (D’Ignazio and Klein 2023). The researchers’ choice to focus on the smaller pay disparity while ignoring the larger retention disparity reveals how quantitative approaches can selectively measure what supports existing power structures while overlooking more significant indicators of systemic problems.\nThis case demonstrates how quantitative methods, when divorced from context and power analysis, can produce misleading conclusions that “justify the status quo rather than challenge it” (Corbett-Davies et al. 2018). The technical transparency of numbers creates an illusion of objectivity that masks underlying value judgments about what constitutes discrimination and what counts as evidence."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#additional-scholarly-perspectives",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#additional-scholarly-perspectives",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Additional Scholarly Perspectives",
    "text": "Additional Scholarly Perspectives\nThree additional scholarly sources provide important perspectives on the tension between quantitative methods and substantive fairness. Corbett-Davies et al. (2018) explore “The Measure and Mismeasure of Fairness” by showing how statistical fairness metrics can produce paradoxical outcomes—situations where satisfying a formal fairness criterion may actually harm the very groups it aims to protect. For example, requiring equal false positive rates across racial groups in pretrial risk assessment might lead to higher overall detention rates for marginalized groups if baseline risk levels differ.\nSelbst et al. (2019) identify five “abstraction traps” in fair ML research where technical fixes fail to account for nuanced social realities. Their insight that technical tools often divorce data from their social origins aligns with Narayanan’s critiques of snapshot datasets and decontextualized analysis. They argue that “fairness” becomes dangerous when reduced to a purely technical problem, divorced from the social contexts that give it meaning.\nGreen (2022) articulates a crucial distinction between “formal” and “substantive” algorithmic fairness. While formal fairness focuses on satisfying statistical criteria, substantive fairness requires examining whether an algorithm contributes to genuine justice outcomes. Green argues that even mathematically “fair” algorithms can encode and perpetuate injustice if they naturalize existing inequalities. This distinction parallels Narayanan’s concern that quantitative methods often identify the wrong locus of intervention.\nData Feminism (D’Ignazio and Klein 2023) offers the most comprehensive framework for addressing Narayanan’s concerns. Their principles of “examine power,” “challenge power,” and “consider context” directly respond to the limitations Narayanan identifies. Through examples like the Anti-Eviction Mapping Project (Chapter 5) and their critique of “Big Dick Data” projects that “ignore context, fetishize size, and inflate their technical and scientific capabilities” (Chapter 6, p. 4), they demonstrate how attention to power dynamics can transform quantitative methods from tools of oppression into instruments of liberation."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#taking-a-position",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#taking-a-position",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Taking a Position",
    "text": "Taking a Position\nIn light of these analyses, I find myself in qualified agreement with Narayanan’s assertion that “quantitative methods are primarily used to justify the status quo” and often “do more harm than good” (Narayanan 2022, 25). The evidence from case studies and scholarly analyses shows how traditional applications of quantitative methods can indeed mask discrimination rather than expose it.\nHowever, my agreement comes with important qualifications. First, Narayanan himself acknowledges that the problem lies not with quantitative methods per se, but with how they are deployed. He states, “If things were different—if the 79 percent of engineers at Google who are male were specifically trained in structural oppression before building their data systems… then their overrepresentation might be very slightly less of a problem” (Narayanan 2022, 13). This suggests that quantitative methods, when embedded in a framework that addresses power and context, could help advance justice.\nSecond, cases like the court appearance study demonstrate that quantitative methods can identify concrete intervention points for reducing disparities. When paired with a critical understanding of systemic injustice, numbers can provide actionable evidence for meaningful reform.\nThe path forward, I believe, requires integrating quantitative precision with what Green (2022) calls “substantive algorithmic fairness”—an approach that goes beyond statistical metrics to consider whether algorithms contribute to substantive justice. This means reversing the null hypothesis to assume discrimination exists unless proven otherwise, collecting longitudinal data that captures compounding effects, focusing on systemic rather than individual-level interventions, engaging directly with affected communities, and treating quantitative evidence as one component in a broader justice framework. By shifting these fundamental assumptions and practices, quantitative methods can become tools for exposing rather than obscuring discrimination.\nAs D’Ignazio and Klein assert, “the data never, ever ‘speak for themselves’” (D’Ignazio and Klein 2023, chap. 6). Acknowledging this truth allows us to use quantitative methods more responsibly—not as neutral arbiters of truth, but as tools that must be wielded with care, context, and a commitment to challenging rather than reinforcing existing power structures."
  },
  {
    "objectID": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#conclusion",
    "href": "posts/Limits_of_the_Quantitative_Approach_to_Bias_and_Fairness/Limits of the Quantitative Approach to Bias and Fairness.html#conclusion",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Conclusion",
    "text": "Conclusion\nNarayanan’s critique of quantitative methods provides a vital caution against simplistic faith in data-driven solutions to discrimination. The limitations he identifies—from problematic null hypotheses to the illusion of objectivity—reveal how seemingly neutral methods can entrench injustice.\nYet certain quantitative approaches, like the court appearance study, demonstrate potential for meaningful reform. The key difference lies not in whether numbers are used, but in how they are contextualized, what questions they seek to answer, and whether they serve to challenge or legitimize existing power structures.\nAs D’Ignazio and Klein remind us in Data Feminism, data work is always political. The question is not whether to use quantitative methods, but how to use them in ways that acknowledge power, consider context, and work toward substantive justice. By approaching quantitative methods with a critical eye and integrating them within a broader commitment to equity, we can transform them from tools that primarily justify the status quo into instruments of lasting change."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This study explores the dynamics of automated decision systems in the bank industry, examining how algorithmic decisions impact both banks and borrowers. Using a dataset of loan applications, I built a logistic regression model to predict default risk and identify an optimal approval threshold that maximizes lender profits. My analysis reveals significant disparities in loan approval rates based on demographic factors and loan purposes. Particularly noteworthy are the variations across age groups, with middle-aged applicants (35-45) receiving the highest approval rates, and across income levels, where higher-income applicants enjoy near-universal approval (96.8%) while lower-income applicants face significantly lower approval rates (55.9%). Additionally, we found that loan intent strongly influences outcomes, with business ventures and education receiving preferential treatment compared to debt consolidation and medical expenses. These findings raise important questions about fairness and access to credit in automated lending systems, highlighting potential areas where algorithmic decision-making may reinforce existing social and economic inequalities."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#introduction",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#introduction",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "This study explores the dynamics of automated decision systems in the bank industry, examining how algorithmic decisions impact both banks and borrowers. Using a dataset of loan applications, I built a logistic regression model to predict default risk and identify an optimal approval threshold that maximizes lender profits. My analysis reveals significant disparities in loan approval rates based on demographic factors and loan purposes. Particularly noteworthy are the variations across age groups, with middle-aged applicants (35-45) receiving the highest approval rates, and across income levels, where higher-income applicants enjoy near-universal approval (96.8%) while lower-income applicants face significantly lower approval rates (55.9%). Additionally, we found that loan intent strongly influences outcomes, with business ventures and education receiving preferential treatment compared to debt consolidation and medical expenses. These findings raise important questions about fairness and access to credit in automated lending systems, highlighting potential areas where algorithmic decision-making may reinforce existing social and economic inequalities."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-a-grab-the-data",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-a-grab-the-data",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part A: Grab the Data",
    "text": "Part A: Grab the Data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the training dataset.\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-b-explore-the-data",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-b-explore-the-data",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part B: Explore The Data",
    "text": "Part B: Explore The Data\nIn this section, we’ll explore patterns in the data by examining how borrower characteristics, loan terms, and approval rates vary across different segments. We’ll focus on three key aspects:\n\nHow borrower characteristics differ by loan purpose\nHow age distribution varies across different loan intents\nThe relationship between homeownership status, interest rates, and loan amounts\n\n\n# Create a summary table of key statistics by loan intent.\nsummary_table = df_train.groupby('loan_intent').agg({\n    'person_age': ['mean', 'median', 'min', 'max'],\n    'person_emp_length': ['mean', 'median'],\n    'loan_int_rate': ['mean', 'median'],\n    'loan_amnt': ['mean', 'median'],\n    'loan_status': ['mean', 'count']\n}).round(2)\n\nprint(\"Summary Statistics by Loan Intent:\")\nsummary_table\n\nSummary Statistics by Loan Intent:\n\n\n\n\n\n\n\n\n\nperson_age\nperson_emp_length\nloan_int_rate\nloan_amnt\nloan_status\n\n\n\nmean\nmedian\nmin\nmax\nmean\nmedian\nmean\nmedian\nmean\nmedian\nmean\ncount\n\n\nloan_intent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n27.59\n26.0\n20\n70\n4.76\n4.0\n10.98\n10.99\n9620.90\n8000.0\n0.29\n4178\n\n\nEDUCATION\n26.60\n25.0\n20\n144\n4.44\n4.0\n10.97\n10.99\n9460.02\n8000.0\n0.17\n5127\n\n\nHOMEIMPROVEMENT\n28.98\n27.0\n21\n60\n5.10\n4.0\n11.16\n11.12\n10348.73\n9000.0\n0.26\n2902\n\n\nMEDICAL\n27.95\n26.0\n20\n94\n4.78\n4.0\n11.05\n10.99\n9242.27\n8000.0\n0.26\n4835\n\n\nPERSONAL\n28.29\n26.0\n20\n144\n4.90\n4.0\n11.01\n10.99\n9549.43\n8000.0\n0.19\n4408\n\n\nVENTURE\n27.59\n26.0\n20\n144\n4.88\n4.0\n10.94\n10.99\n9516.42\n8000.0\n0.15\n4614\n\n\n\n\n\n\n\nThe summary table above provides key insights into borrower profiles by loan intent. Interestingly, education loans attract younger borrowers (early 20s on average), while home improvement and debt consolidation loans are sought by older individuals. Employment length patterns align with these age differences, with education loan applicants showing shorter employment histories, consistent with their likely status as students or recent graduates.\nInterest rates reveal lenders’ risk perceptions across different loan purposes. Medical and personal loans carry higher average interest rates than home improvement or debt consolidation loans, suggesting that lenders view medical and personal expenses as higher-risk lending purposes. These rate differences will be important to consider as we develop our predictive model for default risk.\n\n# Create a boxplot showing age distribution by loan intent.\nplt.figure(figsize=(10, 6))\nloan_intents = df_train['loan_intent'].unique()\ndata_age = [df_train.loc[df_train['loan_intent'] == intent, 'person_age']\n            for intent in loan_intents]\n\nplt.boxplot(data_age, tick_labels=loan_intents)\nplt.title(\"Distribution of Borrowers' Age by Loan Intent\", fontsize=14)\nplt.xlabel(\"Loan Intent\", fontsize=12)\nplt.ylabel(\"Age\", fontsize=12)\nplt.xticks(rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot visualization reinforces and expands on our earlier observations about age distributions. The clear age stratification across loan purposes suggests that financial needs evolve predictably through different life stages. Education loans cluster tightly among younger borrowers, pointing to their relevance primarily during a specific life phase. In contrast, debt consolidation shows a much wider age range, indicating that financial challenges requiring debt restructuring can emerge throughout adulthood.\nMedical and personal loans show more varied age distributions, confirming these needs arise across different life stages. This age-based pattern will be valuable to consider when evaluating our model’s fairness, as we’ll need to ensure that age-based patterns don’t lead to unfair discrimination against certain age groups.\n\n# Create a scatter plot of interest rate vs. loan amount by home ownership status.\nplt.figure(figsize=(10, 6))\nhome_statuses = df_train['person_home_ownership'].unique()\n\ncolors = ['#4b39c5','#39c5bb','#c53943', '#bb39c5']\n\nfor i, status in enumerate(home_statuses):\n    subset = df_train[df_train['person_home_ownership'] == status]\n    plt.scatter(subset['loan_int_rate'], subset['loan_amnt'], color=colors[i],\n                label=status, alpha=0.6, s=50, edgecolor='w',linewidth=0.5)\n\nplt.xlabel(\"Loan Interest Rate (%)\", fontsize=12)\nplt.ylabel(\"Loan Amount ($)\", fontsize=12)\nplt.title(\"Loan Interest Rate vs. Loan Amount by Home Ownership Status\", fontsize=14)\nplt.legend(title=\"Home Ownership\", fontsize=10)\nplt.grid(linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe scatter plot reveals an important trend: borrowers who own homes (OWN or MORTGAGE) generally receive lower interest rates compared to those who rent. This is likely because homeownership serves as a signal of financial stability, making lenders more willing to offer lower rates.\nOn the other hand, renters tend to have higher interest rates, and their loan amounts vary widely, which could indicate a higher perceived risk by lenders. This trend suggests that homeownership status plays a role in determining both the size of credit extended and the interest rates assigned, potentially making it more difficult for renters to access affordable credit."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-c-build-a-model",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-c-build-a-model",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part C: Build a Model",
    "text": "Part C: Build a Model\nNow we’ll build a logistic regression model to predict loan defaults. Our approach will include several preprocessing steps: imputing missing values, standardizing numeric features, and one-hot encoding categorical variables. This preprocessing pipeline will help ensure our model’s robustness.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Select features and target.\nfeatures = [\n    'loan_int_rate', \n    'loan_percent_income', \n    'loan_amnt', \n    'person_emp_length', \n    'person_home_ownership', \n    'loan_intent', \n    'cb_person_default_on_file'\n]\n\nX = df_train[features].copy()\ny = df_train['loan_status']\n\n# Process numeric features.\nnumeric_features = ['loan_int_rate', 'loan_percent_income', 'loan_amnt', 'person_emp_length']\nX_numeric = X[numeric_features].copy()\n\n# For each numeric column, fill missing values with the median, then standardize.\nfor col in numeric_features:\n    median_val = X_numeric[col].median()\n\n    X_numeric[col] = X_numeric[col].fillna(median_val)\n\n    mean_val = X_numeric[col].mean()\n    std_val = X_numeric[col].std()\n\n    X_numeric[col] = (X_numeric[col] - mean_val) / std_val\n\n# Process categorical features.\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\nX_categorical = X[categorical_features].copy()\n\n# For each categorical column, fill missing values with the most frequent value.\nfor col in categorical_features:\n    most_freq = X_categorical[col].value_counts().idxmax()\n    X_categorical[col] = X_categorical[col].fillna(most_freq)\n\n# One-hot encode categorical features using pandas get_dummies.\nX_categorical = pd.get_dummies(X_categorical, prefix=categorical_features)\n\n# Combine numeric and categorical features.\nX_final = pd.concat([X_numeric, X_categorical], axis=1)\n\n# Train logistic regression and evaluate using 5-fold cross-validation.\nclf = LogisticRegression(solver='liblinear', random_state=42)\ncv_scores = cross_val_score(clf, X_final, y, cv=5, scoring='accuracy')\nprint(\"Mean CV Accuracy: {:.2f}%\".format(np.mean(cv_scores) * 100))\n\n# Fit the model on the entire preprocessed training data.\nclf.fit(X_final, y)\nprint(\"Coefficients from the trained model:\")\nprint(clf.coef_)\n\nMean CV Accuracy: 84.63%\nCoefficients from the trained model:\n[[ 0.92933654  1.31348998 -0.57487728 -0.02854312 -0.23295681  0.23958\n  -1.6051062   0.50792713  0.25412267 -0.54438601  0.32701364  0.02861397\n  -0.38820702 -0.76771313 -0.63667484 -0.45388105]]\n\n\nOur logistic regression model achieves an average cross-validation accuracy of approximately 84.6%, indicating strong predictive performance for loan default risk."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-d-find-a-threshold",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-d-find-a-threshold",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part D: Find a Threshold",
    "text": "Part D: Find a Threshold\nWe’ll now determine the optimal threshold for loan approval from the lender’s perspective. This involves calculating the expected profit for different approval thresholds and identifying the one that maximizes average profit per borrower.\n\n# Impute missing interest rates with the median (if not already done)\nmedian_int_rate = df_train['loan_int_rate'].median()\ndf_train['loan_int_rate'] = df_train['loan_int_rate'].fillna(median_int_rate)\n# Prepare for profit calculation using the original columns\nloan_amnt = df_train['loan_amnt'].values\nloan_int_rate = df_train['loan_int_rate'].values / 100.0  # Convert to decimal\ny_actual = df_train['loan_status'].values\n\n# Calculate profit.\nprofit_repaid = loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\nprofit_default = loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\np_default = clf.predict_proba(X_final)[:, 1]\n\n# Sweep thresholds and compute average profit per borrower.\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    approved = p_default &lt; t\n    profit_approved = np.where(y_actual[approved] == 0, profit_repaid[approved], profit_default[approved])\n    total_profit = profit_approved.sum()\n    avg_profit = total_profit / len(df_train)\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\noptimal_idx = np.nanargmax(avg_profits)\noptimal_threshold = thresholds[optimal_idx]\noptimal_profit = avg_profits[optimal_idx]\n\nprint(f\"Optimal threshold: {optimal_threshold:.2f}\")\nprint(f\"Expected profit per borrower at optimal threshold: ${optimal_profit:.2f}\")\nprint(f\"Approval rate at optimal threshold: {(p_default &lt; optimal_threshold).mean():.2%}\")\n\n# Plot the profit vs threshold curve.\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, avg_profits, label='Avg Profit per Borrower', marker='o', markersize=4)\nplt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\nplt.xlabel(\"Threshold (Predicted Default Probability)\", fontsize=12)\nplt.ylabel(\"Average Profit per Borrower ($)\", fontsize=12)\nplt.title(\"Profit per Borrower vs. Decision Threshold\", fontsize=14)\nplt.grid(linestyle='--', alpha=0.7)\nplt.legend(fontsize=11)\nplt.tight_layout()\nplt.show()\n\nOptimal threshold: 0.42\nExpected profit per borrower at optimal threshold: $1386.35\nApproval rate at optimal threshold: 81.86%\n\n\n\n\n\n\n\n\n\nOur profit optimization analysis identifies 0.42 as the optimal threshold for loan approval decisions. At this threshold, the bank maximizes its expected average profit per borrower while maintaining a reasonable approval rate."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-e-evaluating-the-model-from-the-banks-perspective",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-e-evaluating-the-model-from-the-banks-perspective",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part E: Evaluating the Model from the Bank’s Perspective",
    "text": "Part E: Evaluating the Model from the Bank’s Perspective\nWe’ll now validate our model and threshold on a separate test dataset to ensure that the profit estimates are reliable and not just specific to our training data.\nFirst, we’ll preprocess the test dataset following the same steps we applied to our training data.\n\n# Load the test dataset.\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n# Impute missing values.\ndf_test['loan_int_rate'] = df_test['loan_int_rate'].fillna(df_test['loan_int_rate'].median())\ndf_test['person_emp_length'] = df_test['person_emp_length'].fillna(df_test['person_emp_length'].median())\n\n# Select features.\nX_test = df_test[features].copy()\n\n# Process numeric features.\nX_test_numeric = X_test[numeric_features].copy()\nfor col in numeric_features:\n    # Impute missing values with the median.\n    median_val = X_test_numeric[col].median()\n    X_test_numeric[col] = X_test_numeric[col].fillna(median_val)\n    # Standardize the column.\n    mean_val = X_test_numeric[col].mean()\n    std_val = X_test_numeric[col].std()\n    X_test_numeric[col] = (X_test_numeric[col] - mean_val) / std_val\n\n# Process categorical features.\nX_test_categorical = X_test[categorical_features].copy()\nfor col in categorical_features:\n    most_freq = X_test_categorical[col].value_counts().idxmax()\n    X_test_categorical[col] = X_test_categorical[col].fillna(most_freq)\nX_test_categorical = pd.get_dummies(X_test_categorical, prefix=categorical_features)\n\n# Combine numeric and categorical features.\nX_test_final = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n\n# Reindex test data to match training dummy columns.\nX_test_final = X_test_final.reindex(columns=X_final.columns, fill_value=0)\n\nNow that our test data is properly preprocessed, we’ll apply our trained model to generate predictions and calculate expected profits.\n\n# Get loan amount, interest rate, and actual outcomes.\nloan_amnt_test = df_test['loan_amnt'].values\nloan_int_rate_test = df_test['loan_int_rate'].values / 100.0\ny_actual_test = df_test['loan_status'].values \n\n# Compute profit for each loan:\nprofit_repaid_test = loan_amnt_test * (1 + 0.25 * loan_int_rate_test) ** 10 - loan_amnt_test\nprofit_default_test = loan_amnt_test * (1 + 0.25 * loan_int_rate_test) ** 3 - 1.7 * loan_amnt_test\n# Get predicted default probabilities from the trained model.\np_default_test = clf.predict_proba(X_test_final)[:, 1]\n\n# Apply the optimal threshold from the training set to decide approvals.\napproved_test = p_default_test &lt; optimal_threshold\n\n# Compute profit for approved loans.\nprofit_approved_test = (\n    profit_repaid_test[approved_test] * (1 - y_actual_test[approved_test]) +\n    profit_default_test[approved_test] * y_actual_test[approved_test]\n)\n\n# Calculate average profit per borrower.\ntotal_profit_test = profit_approved_test.sum()\navg_profit_test = total_profit_test / len(df_test)\n\nprint(\"Test Set Statistics:\")\nprint(f\"Number of borrowers: {len(df_test)}\")\nprint(f\"Approval rate: {approved_test.mean():.2%}\")\nif approved_test.mean() &gt; 0:\n    print(f\"Default rate among approved loans: {y_actual_test[approved_test].mean():.2%}\")\nelse:\n    print(\"No loans were approved.\")\nprint(f\"Expected profit per borrower: ${avg_profit_test:.2f}\")\n\nTest Set Statistics:\nNumber of borrowers: 6517\nApproval rate: 81.48%\nDefault rate among approved loans: 11.86%\nExpected profit per borrower: $1348.87\n\n\nTo further validate our model, we’ll perform a full threshold analysis on the test set, similar to what we did on the training data. This will let us determine whether the optimal threshold is consistent between training and test datasets, which would indicate strong model generalizability.\n\n# Create an array of threshold values.\nthresholds = np.linspace(0, 1, 101)\navg_profits_test = []\n\n# For each threshold, calculate average profit per borrower.\nfor t in thresholds:\n    approved_temp = p_default_test &lt; t\n    profit_approved_temp = (\n        profit_repaid_test[approved_temp] * (1 - y_actual_test[approved_temp]) +\n        profit_default_test[approved_temp] * y_actual_test[approved_temp]\n    )\n    total_profit_temp = profit_approved_temp.sum()\n    avg_profit_temp = total_profit_temp / len(df_test)\n    avg_profits_test.append(avg_profit_temp)\n\navg_profits_test = np.array(avg_profits_test)\noptimal_idx_test = np.argmax(avg_profits_test)\noptimal_threshold_test = thresholds[optimal_idx_test]\noptimal_profit_test = avg_profits_test[optimal_idx_test]\n\nprint(\"\\nThreshold Comparison:\")\nprint(f\"Optimal threshold on training set: {optimal_threshold:.2f}\")\nprint(f\"Optimal threshold on test set: {optimal_threshold_test:.2f}\")\nprint(f\"Training set expected profit per borrower: ${optimal_profit:.2f}\")\nprint(f\"Test set expected profit per borrower: ${optimal_profit_test:.2f}\")\n\n# Plot Profit Curves for Training and Test Sets.\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, avg_profits, marker='o', markersize=4, linestyle='-', alpha=0.7, label='Training Set')\nplt.plot(thresholds, avg_profits_test, marker='s', markersize=4, linestyle='-', alpha=0.7, label='Test Set')\nplt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Training Optimal: {optimal_threshold:.2f}')\nplt.axvline(optimal_threshold_test, color='green', linestyle='--', label=f'Test Optimal: {optimal_threshold_test:.2f}')\nplt.xlabel(\"Threshold (Predicted Default Probability)\", fontsize=12)\nplt.ylabel(\"Average Profit per Borrower ($)\", fontsize=12)\nplt.title(\"Comparing Profit Curves: Training vs. Test Set\", fontsize=14)\nplt.grid(linestyle='--', alpha=0.7)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\nThreshold Comparison:\nOptimal threshold on training set: 0.42\nOptimal threshold on test set: 0.35\nTraining set expected profit per borrower: $1386.35\nTest set expected profit per borrower: $1367.62\n\n\n\n\n\n\n\n\n\nOur model’s performance on the test set is very encouraging. The optimal threshold found on the test set is close to the one we determined using the training data, and the expected profits are similar between the two datasets. This consistency suggests that our model generalizes well to new data and that the profit estimates are reliable. The expected profit per borrower on the test set is approximately $1367.62. These figures are very close to the training set’s expected profit per borrower of about $1386.35."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-f-evaluating-the-model-from-the-borrowers-perspective",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-f-evaluating-the-model-from-the-borrowers-perspective",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part F: Evaluating the Model from the Borrower’s Perspective",
    "text": "Part F: Evaluating the Model from the Borrower’s Perspective\nNow we’ll analyze how the model’s decisions affect different groups of borrowers, focusing on approval rates by age, loan purpose, and income level.\n\n# Analysis by Loan Intent\ndf_test['approved'] = p_default_test &lt; optimal_threshold\nintent_summary = df_test.groupby('loan_intent', observed=False).agg(\n    total_applicants=('approved', 'count'),\n    approved_count=('approved', 'sum'),\n    default_rate=('loan_status', 'mean')\n).reset_index()\nintent_summary['approval_rate'] = intent_summary['approved_count'] / intent_summary['total_applicants']\n\n# Create age groups\ndf_test['age_group'] = pd.cut(df_test['person_age'], \n                              bins=[0, 25, 35, 45, 55, 100],\n                              labels=['&lt;25', '25-35', '35-45', '45-55', '55+'])\n\n# Compute approval and default rates by age group\nage_summary = df_test.groupby('age_group', observed=False).agg(\n    total_applicants=('loan_status', 'count'),\n    approved_count=('approved', 'sum'),\n    default_rate=('loan_status', 'mean')\n).reset_index()\nage_summary['approval_rate'] = age_summary['approved_count'] / age_summary['total_applicants']\n\n# Default rate among approved applicants by age group\napproved_defaults_by_age = df_test[df_test['approved']].groupby('age_group', observed=False).agg(\n    approved_default_rate=('loan_status', 'mean')\n).reset_index()\nage_summary = age_summary.merge(approved_defaults_by_age, on='age_group', how='left')\n\n# Default rate among approved applicants by loan intent\napproved_defaults_by_intent = df_test[df_test['approved']].groupby('loan_intent', observed=False).agg(\n    approved_default_rate=('loan_status', 'mean'),\n    approved_applicants=('loan_status', 'count')\n).reset_index()\nintent_summary = intent_summary.merge(approved_defaults_by_intent, on='loan_intent', how='left')\n\n# Sort by approval rate for better visualization\nintent_summary = intent_summary.sort_values('approval_rate', ascending=False)\n\n# Analysis by Income Level\ndf_test['income_bin'] = pd.qcut(df_test['person_income'], q=4, \n                                labels=['Low', 'Medium', 'High', 'Very High'])\n\nincome_summary = df_test.groupby('income_bin', observed=False).agg(\n    total_applicants=('approved', 'count'),\n    approved_count=('approved', 'sum'),\n    default_rate=('loan_status', 'mean'),\n    median_income=('person_income', 'median')\n).reset_index()\nincome_summary['approval_rate'] = income_summary['approved_count'] / income_summary['total_applicants']\n\n# Default rate among approved applicants by income level\napproved_defaults_by_income = df_test[df_test['approved']].groupby('income_bin', observed=False).agg(\n    approved_default_rate=('loan_status', 'mean')\n).reset_index()\nincome_summary = income_summary.merge(approved_defaults_by_income, on='income_bin', how='left')\n\n# Print table: Approval and Default Rates by Loan Intent\nprint(\"Approval and Default Rates by Loan Intent:\")\nprint(intent_summary[['loan_intent', 'total_applicants', 'approval_rate', \n                      'default_rate', 'approved_default_rate']].to_string(index=False))\nprint(\"\\n\")\n\n# Print table: Approval and Default Rates by Age Group\nprint(\"Approval and Default Rates by Age Group:\")\nprint(age_summary[['age_group', 'total_applicants', 'approval_rate', \n                   'default_rate', 'approved_default_rate']].to_string(index=False))\nprint(\"\\n\")\n\n# Print table: Approval and Default Rates by Income Level\nprint(\"Approval and Default Rates by Income Level:\")\nprint(income_summary[['income_bin', 'median_income', 'total_applicants', \n                      'approval_rate', 'default_rate', 'approved_default_rate']].to_string(index=False))\n\n# Create visualizations for these results\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Plot 1: Approval rates by age group\naxes[0].bar(age_summary['age_group'], age_summary['approval_rate'] * 100, color='skyblue')\naxes[0].set_title('Approval Rates by Age Group', fontsize=14)\naxes[0].set_xlabel('Age Group', fontsize=12)\naxes[0].set_ylabel('Approval Rate (%)', fontsize=12)\naxes[0].set_ylim(0, 100)\naxes[0].grid(axis='y', linestyle='--', alpha=0.7)\nfor i, v in enumerate(age_summary['approval_rate']):\n    axes[0].text(i, v * 100 + 2, f\"{v:.1%}\", ha='center')\n\n# Plot 2: Approval rates by loan intent\nintent_summary_sorted = intent_summary.sort_values('approval_rate')\naxes[1].barh(intent_summary_sorted['loan_intent'], intent_summary_sorted['approval_rate'] * 100, color='lightgreen')\naxes[1].set_title('Approval Rates by Loan Intent', fontsize=14)\naxes[1].set_xlabel('Approval Rate (%)', fontsize=12)\naxes[1].set_xlim(0, 100)\naxes[1].grid(axis='x', linestyle='--', alpha=0.7)\nfor i, v in enumerate(intent_summary_sorted['approval_rate']):\n    axes[1].text(v * 100 + 2, i, f\"{v:.1%}\", va='center')\n\n# Plot 3: Approval rates by income level\naxes[2].bar(income_summary['income_bin'], income_summary['approval_rate'] * 100, color='salmon')\naxes[2].set_title('Approval Rates by Income Level', fontsize=14)\naxes[2].set_xlabel('Income Quartile', fontsize=12)\naxes[2].set_ylabel('Approval Rate (%)', fontsize=12)\naxes[2].set_ylim(0, 100)\naxes[2].grid(axis='x', linestyle='--', alpha=0.7)\nfor i, v in enumerate(income_summary['approval_rate']):\n    axes[2].text(i, v * 100 + 2, f\"{v:.1%}\", va='center')\n\nplt.tight_layout()\nplt.show()\n\nApproval and Default Rates by Loan Intent:\n      loan_intent  total_applicants  approval_rate  default_rate  approved_default_rate\n          VENTURE              1105       0.890498      0.145701               0.073171\n        EDUCATION              1326       0.868778      0.167421               0.103299\n         PERSONAL              1113       0.833783      0.219227               0.132543\n  HOMEIMPROVEMENT               703       0.789474      0.246088               0.153153\n          MEDICAL              1236       0.760518      0.281553               0.146809\nDEBTCONSOLIDATION              1034       0.726306      0.279497               0.123835\n\n\nApproval and Default Rates by Age Group:\nage_group  total_applicants  approval_rate  default_rate  approved_default_rate\n      &lt;25              3075       0.799024      0.227967               0.111518\n    25-35              2727       0.827283      0.214155               0.123227\n    35-45               596       0.845638      0.204698               0.128968\n    45-55                95       0.800000      0.221053               0.131579\n      55+                23       0.695652      0.391304               0.187500\n\n\nApproval and Default Rates by Income Level:\nincome_bin  median_income  total_applicants  approval_rate  default_rate  approved_default_rate\n       Low        30000.0              1648       0.617112      0.390777               0.193707\n    Medium        47815.5              1658       0.780458      0.221351               0.108192\n      High        66000.0              1659       0.893309      0.162749               0.107962\n Very High       105602.0              1552       0.977448      0.100515               0.087673\n\n\n\n\n\n\n\n\n\n\nAnalysis of Borrower Impact Based on the Decision System\n\n1. Approval Rates by Age Group\nThe results indicate that younger applicants (&lt;25) and older applicants (55+) have lower approval rates compared to individuals in their prime working years (25-45). The highest approval rate is observed in the 35-45 age group (84.6%), while applicants under 25 (79.9%) and over 55 (69.6%) face a harder time getting approved for loans. This trend suggests that age is a factor in loan approval, likely due to risk perception—younger borrowers may be seen as financially inexperienced with lower credit histories, while older applicants may be perceived as closer to retirement, affecting their ability to repay long-term loans.\n\n\n2. Approval and Default Rates by Loan Intent\nApproval rates vary significantly based on the purpose of the loan. Venture loans have the highest approval rate (89.0%), possibly because businesses are seen as creditworthy investments with potential revenue generation. Education loans are also highly approved (86.9%), indicating a strong willingness to fund students, likely because they are expected to earn more in the future. Debt consolidation loans have the lowest approval rate (72.6%), reflecting lender caution towards individuals trying to restructure debt, which may signal financial distress. Medical loans (76.1%) and personal loans (79.0%) fall in the middle, showing moderate approval rates but also relatively high default rates (14.7% and 13.3%) among approved borrowers. Looking at default rates among approved borrowers, we see that venture loans have the lowest default rate (7.3%) while home improvement loans show the highest (15.3%). This suggests that borrowers taking loans for home improvements or medical expenses may be at a higher risk of default, potentially due to unforeseen financial strain.\n\n\n3. Approval Rates by Income Level\nAs expected, income plays a strong role in determining loan approvals. Borrowers in the lowest income quartile have the lowest approval rate (61.7%), suggesting that financial stability is a key factor in decision-making. The approval rate rises progressively across income groups, with the highest-income borrowers seeing a 97.7% approval rate, indicating that lenders heavily favor those with stronger financial backing. This trend highlights a systemic bias toward wealthier individuals, raising concerns about accessibility to credit for lower-income applicants who might genuinely need financial assistance. These findings raise important ethical and policy questions about fairness in algorithmic lending decisions, particularly regarding how different groups of borrowers are affected by the model’s optimization for lender profit."
  },
  {
    "objectID": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-g-conclusions",
    "href": "posts/Design_and_Impact_of_Automated_Decision_Systems/Design_and_Impact_of_Automated_Decision_Systems.html#part-g-conclusions",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Part G: Conclusions",
    "text": "Part G: Conclusions\nConsidering that people seeking loans for medical expenses have high rates of default (14.7%), the question arises whether it is fair that they face greater difficulty obtaining credit. I define fairness here as equal treatment of individuals with equal risk profiles, combined with consideration of the causal factors behind apparent risk and the broader societal context of the decision. While the statistical model optimizes for profit by identifying genuine default risks, this approach fails to consider why medical loan applicants default more frequently. Medical expenses are often unexpected, unavoidable, and connected to circumstances beyond an individual’s control. Unlike education or business loans, which represent investments with potential returns, medical loans typically address immediate health needs. A system that restricts access to credit for medical purposes may be statistically justified but ethically problematic, as it compounds vulnerability by restricting financial resources precisely when individuals face health crises. A truly fair system would incorporate these contextual factors, perhaps through different thresholds for medical loans or alternative criteria that acknowledge the unique nature of healthcare financing needs.\nThis analysis has illuminated how automated decision systems, while appearing objective, can reinforce existing societal inequalities by encoding patterns present in historical data. The stark approval disparities across income levels likely reflect genuine default risk differences, but they also reveal how algorithmic lending decisions may exacerbate wealth gaps by making credit more accessible to those who already enjoy financial stability. The model’s treatment of different loan purposes also demonstrates how context-blind optimization for profit can lead to socially suboptimal outcomes, particularly in cases like medical loans where default may stem from structural factors rather than individual financial responsibility. Additionally, the age-based disparities highlight how life-stage vulnerabilities can be magnified through algorithmic decision-making. These findings underscore the importance of considering not just model accuracy but also the broader implications of automated decisions on different segments of society. Moving forward, lending institutions should consider incorporating fairness metrics alongside profit maximization, potentially through modified thresholds for vulnerable groups, additional contextual variables, or supplementary human review processes that can identify cases where strict algorithmic decisions might cause undue harm."
  },
  {
    "objectID": "posts/Implementing_Logistic_Regression/logistic_regression.html",
    "href": "posts/Implementing_Logistic_Regression/logistic_regression.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Implementing_Logistic_Regression/logistic_regression.html#source-code",
    "href": "posts/Implementing_Logistic_Regression/logistic_regression.html#source-code",
    "title": "Implementing Logistic Regression",
    "section": "Source Code",
    "text": "Source Code\nThe complete logistic regression implementation can be found in my Github repository."
  },
  {
    "objectID": "posts/Implementing_Logistic_Regression/logistic_regression.html#abstract",
    "href": "posts/Implementing_Logistic_Regression/logistic_regression.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement logistic regression with gradient descent optimization using PyTorch. I explore the benefits of gradient descent with momentum compared to vanilla gradient descent, demonstrating how momentum accelerates convergence by accumulating velocity in consistent gradient directions. Through four experiments, I investigate:\n\nthe convergence properties of vanilla gradient descent\nthe acceleration benefits of momentum\nthe overfitting phenomenon in high-dimensional spaces\nthe practical application of both optimization methods on the Breast Cancer Wisconsin dataset.\n\nThe results clearly demonstrate that gradient descent with momentum converges faster and achieves better final performance than vanilla gradient descent."
  },
  {
    "objectID": "posts/Implementing_Logistic_Regression/logistic_regression.html#experiments",
    "href": "posts/Implementing_Logistic_Regression/logistic_regression.html#experiments",
    "title": "Implementing Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\nHere are the helping methods:\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\ndef plot_decision_boundary(X, y, model, ax=None, title=None):\n    if ax is None:\n        ax = plt.subplots(figsize=(8, 6))\n    \n    # Plot data points\n    ax.scatter(X[:, 0][y == 0], X[:, 1][y == 0], color='blue', label='Class 0', alpha=0.5)\n    ax.scatter(X[:, 0][y == 1], X[:, 1][y == 1], color='red', label='Class 1', alpha=0.5)\n    \n    # Create a grid to plot decision boundary\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n    \n    # Create features for grid points (add constant 1)\n    grid = np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape[0])]\n    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n    \n    # Get predictions\n    Z = model.predict(grid_tensor).detach().numpy()\n    Z = Z.reshape(xx.shape)\n    \n    # Plot decision boundary\n    ax.contour(xx, yy, Z, levels=[0.5], colors='black')\n    \n    # Add labels and title\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    if title:\n        ax.set_title(title)\n    ax.legend()\n    \n    return ax\n\n\nExperiment 1: Vanilla gradient descent\nThis experiment aims to demonstrate the fundamental behavior of vanilla gradient descent for logistic regression on a simple binary classification problem. The goal is to show that when the learning rate (α) is properly chosen and the dataset is relatively simple, gradient descent will converge smoothly to a solution that effectively separates the two classes.\n\n# Generate data\nX, y = classification_data(n_points=300, noise=0.3)\n\n# Initialize model and optimizer\nlr = LogisticRegression()\nopt = GradientDescentOptimizer(lr)\n\n# Training loop\nlosses = []\nn_iterations = 200\n\nfor i in range(n_iterations):\n    loss = lr.loss(X, y)\n    losses.append(loss.item())\n    \n    opt.step(X, y, alpha=0.1, beta=0)\n\n# Plot loss\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration (Vanilla Gradient Descent)')\n\n# Plot decision boundary\nax = plt.subplot(1, 2, 2)\nplot_decision_boundary(X, y, lr, ax, title='Vanilla Gradient Descent Decision Boundary')\n\nplt.tight_layout()\nplt.show()\n\n/var/folders/9f/x536p69s5px424c7twrmgtp40000gn/T/ipykernel_78698/2787871557.py:33: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n/var/folders/9f/x536p69s5px424c7twrmgtp40000gn/T/ipykernel_78698/2787871557.py:34: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n  np.arange(y_min, y_max, 0.01))\n\n\n\n\n\n\n\n\n\nFigure 1 demonstrates that vanilla gradient descent successfully converges on the classification task. The left plot shows the loss decreasing monotonically over 200 iterations, starting at around 0.55 and dropping to about 0.20. The right plot displays the decision boundary as a clean linear separator between the two classes (blue and red), showing that the model effectively learned to distinguish between them.\n\n\nExperiment 2: Benefits of momentum\nThis experiment compares vanilla gradient descent with gradient descent using momentum to highlight the acceleration benefits that momentum provides during optimization. Momentum works by accumulating a velocity vector in directions of consistent gradient, which helps overcome plateaus in the loss landscape and dampens oscillations in directions with high curvature.\n\nX, y = classification_data(n_points=300, noise=0.3)\n\n# Initialize models and optimizers\nlr_vanilla = LogisticRegression()\nopt_vanilla = GradientDescentOptimizer(lr_vanilla)\n\nlr_momentum = LogisticRegression()\nopt_momentum = GradientDescentOptimizer(lr_momentum)\n\n# Training loop for vanilla GD\nlosses_vanilla = []\n\n# Training loop for GD with momentum\nlosses_momentum = []\n\nn_iterations = 100\n\nfor i in range(n_iterations):\n    # Vanilla GD\n    loss_vanilla = lr_vanilla.loss(X, y)\n    losses_vanilla.append(loss_vanilla.item())\n    opt_vanilla.step(X, y, alpha=0.2, beta=0)\n    \n    # GD with momentum\n    loss_momentum = lr_momentum.loss(X, y)\n    losses_momentum.append(loss_momentum.item())\n    opt_momentum.step(X, y, alpha=0.2, beta=0.9)\n\n# Plot comparison of losses\nplt.figure(figsize=(12, 5))\n\nplt.plot(losses_vanilla, label='Vanilla GD')\nplt.plot(losses_momentum, label='GD with Momentum')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss Comparison: Vanilla GD vs. GD with Momentum')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2 compares vanilla gradient descent with gradient descent using momentum. The momentum-based approach (orange line) converges much faster than vanilla GD (blue line). While both methods start at similar loss values (0.55-0.60), the momentum approach reaches a loss of 0.1 in just 20 iterations, whereas vanilla GD requires about 80 iterations to reach 0.2. By the end of training, momentum achieves a significantly lower final loss (~0.02 vs ~0.17).\n\n\nExperiment 3: Overfitting\nThis experiment explores the fundamental problem of overfitting in machine learning, which occurs when a model learns the training data too well, including its noise and peculiarities, at the expense of generalization to new data. We deliberately create a challenging scenario with 100 features but only 50 data points to demonstrate this phenomenon.\n\n# Set dimensions\np_dim = 100\nn_points = 50\n\n# Generate two datasets with identical parameters\ndef generate_high_dim_data(n_points, p_dim):\n    # Generate binary labels\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n\n    X = torch.normal(0.0, 1.0, size=(n_points, p_dim-1))\n\n    signal = 0.2 * y[:, None] * torch.ones((n_points, p_dim-1))\n    X = X + signal\n\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX_train, y_train = generate_high_dim_data(n_points, p_dim)\nX_test, y_test = generate_high_dim_data(n_points, p_dim)\n\n# Initialize model and optimizer\nlr = LogisticRegression()\nopt = GradientDescentOptimizer(lr)\n\n# Training loop\ntrain_losses = []\ntrain_accuracies = []\ntest_accuracies = []\n\nn_iterations = 1000\n\nfor i in range(n_iterations):\n    # Calculate and store loss\n    loss = lr.loss(X_train, y_train)\n    train_losses.append(loss.item())\n    \n    # Calculate and store training accuracy\n    train_preds = lr.predict(X_train)\n    train_acc = (train_preds == y_train).float().mean().item()\n    train_accuracies.append(train_acc)\n    \n    # Calculate and store test accuracy\n    test_preds = lr.predict(X_test)\n    test_acc = (test_preds == y_test).float().mean().item()\n    test_accuracies.append(test_acc)\n    \n    # Update weights with momentum\n    opt.step(X_train, y_train, alpha=0.02, beta=0.9)\n\n# Plot results\nplt.figure(figsize=(15, 5))\n\n# Plot loss\nplt.subplot(1, 3, 1)\nplt.plot(train_losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss')\n\n# Plot training accuracy\nplt.subplot(1, 3, 2)\nplt.plot(train_accuracies)\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.ylim(0, 1.05)\n\n# Plot test accuracy\nplt.subplot(1, 3, 3)\nplt.plot(test_accuracies)\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.title('Test Accuracy')\nplt.ylim(0, 1.05)\n\nplt.tight_layout()\nplt.show()\n\nfinal_train_acc = train_accuracies[-1]\nfinal_test_acc = test_accuracies[-1]\n\nprint(f\"Final training accuracy: {final_train_acc:.4f}\")\nprint(f\"Final test accuracy: {final_test_acc:.4f}\")\n\n\n\n\n\n\n\n\nFinal training accuracy: 1.0000\nFinal test accuracy: 0.5800\n\n\nThis figure clearly demonstrates the overfitting phenomenon. From left to right:\n\nTraining loss rapidly decreases and approaches zero\nTraining accuracy quickly reaches 100% and remains there\nTest accuracy initially rises but then stabilizes around 58%\n\nThe final metrics confirm this: 100% training accuracy but only 58% test accuracy, showing that the model memorized the training data but failed to generalize well to new examples.\n\n\nExperiment 4: Performance on empirical data\nThis experiment applies our logistic regression implementation to the Breast Cancer Wisconsin dataset, demonstrating the practical utility of our algorithm on a real-world medical classification problem. This dataset is particularly meaningful as an application of machine learning, as accurate classification of benign versus malignant breast masses can assist medical professionals in diagnosis and treatment planning.\nFor this experiment, we used the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository. This dataset was originally compiled by Dr. William H. Wolberg, Dr. W. Nick Street, and Olvi L. Mangasarian at the University of Wisconsin. The dataset contains features computed from digitized images of fine needle aspirates (FNA) of breast masses, describing characteristics of cell nuclei present in the images. Each instance represents measurements from one patient case, with 30 numerical features including radius, texture, perimeter, area, smoothness, and other attributes of the cell nuclei. The target variable indicates whether the breast mass is benign (0) or malignant (1), making it suitable for binary classification.\n\n# Load breast cancer dataset\ndata = load_breast_cancer()\nX_np = data.data\ny_np = data.target\n\n# Normalize features\nX_np = (X_np - X_np.mean(axis=0)) / X_np.std(axis=0)\n\n# Create train, validation, and test splits\nX_train_np, X_temp_np, y_train_np, y_temp_np = train_test_split(\n    X_np, y_np, test_size=0.4, random_state=42)\n\nX_val_np, X_test_np, y_val_np, y_test_np = train_test_split(\n    X_temp_np, y_temp_np, test_size=0.5, random_state=42)\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train_np, dtype=torch.float32)\ny_train = torch.tensor(y_train_np, dtype=torch.float32)\n\nX_val = torch.tensor(X_val_np, dtype=torch.float32)\ny_val = torch.tensor(y_val_np, dtype=torch.float32)\n\nX_test = torch.tensor(X_test_np, dtype=torch.float32)\ny_test = torch.tensor(y_test_np, dtype=torch.float32)\n\n# Add constant feature to each dataset\nX_train = torch.cat((X_train, torch.ones((X_train.shape[0], 1))), 1)\nX_val = torch.cat((X_val, torch.ones((X_val.shape[0], 1))), 1)\nX_test = torch.cat((X_test, torch.ones((X_test.shape[0], 1))), 1)\n\n# Train with vanilla GD\nlr_vanilla = LogisticRegression()\nopt_vanilla = GradientDescentOptimizer(lr_vanilla)\n\n# Train with momentum\nlr_momentum = LogisticRegression()\nopt_momentum = GradientDescentOptimizer(lr_momentum)\n\n# Training loop\ntrain_losses_vanilla = []\nval_losses_vanilla = []\n\ntrain_losses_momentum = []\nval_losses_momentum = []\n\nn_iterations = 200\n\nfor i in range(n_iterations):\n    # Vanilla GD\n    train_loss_vanilla = lr_vanilla.loss(X_train, y_train)\n    val_loss_vanilla = lr_vanilla.loss(X_val, y_val)\n    \n    train_losses_vanilla.append(train_loss_vanilla.item())\n    val_losses_vanilla.append(val_loss_vanilla.item())\n    \n    opt_vanilla.step(X_train, y_train, alpha=0.01, beta=0)\n    \n    # GD with momentum\n    train_loss_momentum = lr_momentum.loss(X_train, y_train)\n    val_loss_momentum = lr_momentum.loss(X_val, y_val)\n    \n    train_losses_momentum.append(train_loss_momentum.item())\n    val_losses_momentum.append(val_loss_momentum.item())\n    \n    opt_momentum.step(X_train, y_train, alpha=0.01, beta=0.9)\n\n# Plot training and validation losses\nplt.figure(figsize=(12, 5))\n\n# Vanilla GD losses\nplt.subplot(1, 2, 1)\nplt.plot(train_losses_vanilla, label='Training Loss')\nplt.plot(val_losses_vanilla, label='Validation Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Vanilla GD: Training and Validation Loss')\nplt.legend()\n\n# Momentum GD losses\nplt.subplot(1, 2, 2)\nplt.plot(train_losses_momentum, label='Training Loss')\nplt.plot(val_losses_momentum, label='Validation Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('GD with Momentum: Training and Validation Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Evaluate on test set\ntest_loss_vanilla = lr_vanilla.loss(X_test, y_test).item()\ntest_preds_vanilla = lr_vanilla.predict(X_test)\ntest_acc_vanilla = (test_preds_vanilla == y_test).float().mean().item()\n\ntest_loss_momentum = lr_momentum.loss(X_test, y_test).item()\ntest_preds_momentum = lr_momentum.predict(X_test)\ntest_acc_momentum = (test_preds_momentum == y_test).float().mean().item()\n\nprint(\"Test Set Results:\")\nprint(f\"Vanilla GD - Loss: {test_loss_vanilla:.4f}, Accuracy: {test_acc_vanilla:.4f}\")\nprint(f\"GD with Momentum - Loss: {test_loss_momentum:.4f}, Accuracy: {test_acc_momentum:.4f}\")\n\n\n\n\n\n\n\n\nTest Set Results:\nVanilla GD - Loss: 0.2713, Accuracy: 0.9123\nGD with Momentum - Loss: 0.0582, Accuracy: 0.9912\n\n\nThis figure shows the performance comparison on real-world data. The momentum-based approach (right) converges much faster than vanilla GD (left), reaching low loss values after just 25 iterations. Both training and validation losses decrease together, which indicates good generalization. The provided metrics confirm this: GD with momentum achieved 98.25% test accuracy with a low loss of 0.0663, significantly outperforming vanilla GD’s 89.47% accuracy and 0.2854 loss."
  },
  {
    "objectID": "posts/Implementing_Logistic_Regression/logistic_regression.html#discussion",
    "href": "posts/Implementing_Logistic_Regression/logistic_regression.html#discussion",
    "title": "Implementing Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I implemented logistic regression with gradient descent optimization from scratch and conducted four experiments to evaluate its performance. I demonstrated that vanilla gradient descent consistently decreases the loss and converges to an effective decision boundary when properly tuned. I then showed that gradient descent with momentum dramatically accelerates convergence, reducing the number of iterations needed by approximately 4 times compared to vanilla gradient descent. The third experiment illustrated the overfitting phenomenon, where the model achieved perfect accuracy on high-dimensional training data but only 58% on test data, highlighting the dangers of having more features than samples. Finally, I applied both optimization methods to the Breast Cancer Wisconsin dataset, where gradient descent with momentum achieved 99.12% accuracy compared to vanilla gradient descent’s 91.23%. Through these experiments, I gained a deeper understanding of the mathematics behind gradient descent optimization, the impact of momentum on convergence speed, and the practical considerations when applying these techniques to real-world data. This implementation serves as a foundation for more complex optimization algorithms and neural network architectures."
  },
  {
    "objectID": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html",
    "href": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron_model import Perceptron, PerceptronOptimizer\nfrom minibatch import MiniBatchPerceptron, MiniBatchPerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#link-to-source-code",
    "href": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#link-to-source-code",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Link to Source Code",
    "text": "Link to Source Code\nThe complete perceptron implementation can be found in my Github repository."
  },
  {
    "objectID": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#implementation-walkthrough",
    "href": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#implementation-walkthrough",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Implementation Walkthrough",
    "text": "Implementation Walkthrough\nThe heart of the perceptron algorithm lies in the grad() method, which accurately implements the mathematical update rule:\ndef grad(self, X, y):\n    # Convert y from {0,1} to {-1,1} for perceptron math\n    y_ = 2*y - 1\n    \n    # Compute scores\n    scores = self.score(X)\n    \n    # Identify misclassified points\n    misclassified = (scores * y_ &lt;= 0).float()\n    \n    # Calculate update vector using vectorized operations\n    return -torch.sum(misclassified.unsqueeze(1) * y_.unsqueeze(1) * X, dim=0)\nThis implementation directly translates the formula \\(-1[s_i(2y_i-1) &lt; 0](2y_i-1)x_i\\) by: 1. Converting labels to {-1, 1} format 2. Computing whether each point is misclassified using the sign comparison 3. Calculating the update vector in a vectorized manner\nTo check my implementation on the mini loop, first, let’s set up the imports and data generation:\n\nimport torch\nfrom matplotlib import pyplot as plt\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Set random seed for reproducibility\ntorch.manual_seed(1234)\n\n# Function to generate perceptron data\ndef perceptron_data(n_points=300, noise=0.2, p_dims=2):\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\n# Function to plot perceptron data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s=20, c=2*y[ix]-1, facecolors=\"none\", \n                  edgecolors=\"darkgrey\", cmap=\"BrBG\", vmin=-2, vmax=2, \n                  alpha=0.5, marker=markers[i])\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\n# Function to draw a decision boundary line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nNext, let’s generate and visualize the data:\n\n# Generate 2D data with some noise\nX, y = perceptron_data(n_points=50, noise=0.3)\n\n# Visualize the data\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.set(xlim=(-1, 2), ylim=(-1, 2))\nplot_perceptron_data(X, y, ax)\nplt.title(\"Perceptron Data\")\nplt.show()\n\n\n\n\n\n\n\n\nNow, let’s run the perceptron training loop and visualize the results:\n\n# Instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0  # Initial loss\nmax_iter = 1000  # Safety limit\niter_count = 0\n\n# For keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\n# Training loop\nwhile loss &gt; 0 and iter_count &lt; max_iter:  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # Pick a random data point\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # Perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    \n    iter_count += 1\n\nprint(f\"Final loss: {loss:.4f}\")\nprint(f\"Number of iterations: {iter_count}\")\n\nFinal loss: 0.0000\nNumber of iterations: 65\n\n\nThe perceptron algorithm achieves Loss = 0, indicating that it has perfectly classified the training data.\nLet’s visualize the training progress:\n\n# Plot the loss over iterations\nplt.figure(figsize=(8, 4))\nplt.plot(loss_vec, color=\"slategrey\")\nplt.scatter(np.arange(len(loss_vec)), loss_vec, color=\"slategrey\")\nplt.xlabel(\"Perceptron Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Perceptron Training Progress\")\nplt.grid(True)\nplt.show()\n\n# Plot the final decision boundary\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.set(xlim=(-1, 2), ylim=(-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color=\"black\", linewidth=2)\nplt.title(\"Perceptron Decision Boundary After Training\")\nplt.show()"
  },
  {
    "objectID": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#experiment-1-minibatch-with-k1",
    "href": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#experiment-1-minibatch-with-k1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 1: Minibatch with k=1",
    "text": "Experiment 1: Minibatch with k=1\n\nX1, y1 = generate_linearly_separable_data(n_points=100, margin=0.2)\nbatch_size = 1\np1, loss_vec1, iter_count1 = train_minibatch_perceptron(X1, y1, batch_size=batch_size)\n\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Final loss: {loss_vec1[-1]:.4f}\")\nprint(f\"Iterations required: {iter_count1}\")\n\n# Visualize Experiment 1\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nplot_perceptron_data(X1, y1, ax1)\ndraw_line(p1.w, -1.5, 1.5, ax1, color=\"black\", linewidth=2)\nax1.set_title(f\"Decision Boundary with Batch Size k={batch_size}\")\nax1.set(xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\n\nax2.plot(loss_vec1, color=\"purple\", linewidth=2)\nax2.scatter(np.arange(len(loss_vec1)), loss_vec1, color=\"purple\", s=20)\nax2.set_xlabel(\"Iteration\")\nax2.set_ylabel(\"Loss\")\nax2.set_title(f\"Loss Evolution with Batch Size k={batch_size}\")\nax2.grid(True)\nplt.tight_layout()\nplt.show()\n\nBatch size: 1\nFinal loss: 0.0000\nIterations required: 122\n\n\n\n\n\n\n\n\n\nWhen the batch size k=1, the minibatch perceptron is equivalent to the standard perceptron.\nKey observations: - Updates are based on individual data points - Convergence is guaranteed for linearly separable data - The path to convergence can be irregular and depend on the random order of samples"
  },
  {
    "objectID": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#experiment-2-minibatch-with-k10",
    "href": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#experiment-2-minibatch-with-k10",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 2: Minibatch with k=10",
    "text": "Experiment 2: Minibatch with k=10\n\nX2, y2 = generate_linearly_separable_data(n_points=100, margin=0.2)\nbatch_size = 10\np2, loss_vec2, iter_count2 = train_minibatch_perceptron(X2, y2, batch_size=batch_size)\n\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Final loss: {loss_vec2[-1]:.4f}\")\nprint(f\"Iterations required: {iter_count2}\")\n\n# Visualize Experiment 2\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nplot_perceptron_data(X2, y2, ax1)\ndraw_line(p2.w, -1.5, 1.5, ax1, color=\"black\", linewidth=2)\nax1.set_title(f\"Decision Boundary with Batch Size k={batch_size}\")\nax1.set(xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\n\nax2.plot(loss_vec2, color=\"teal\", linewidth=2)\nax2.scatter(np.arange(len(loss_vec2)), loss_vec2, color=\"teal\", s=20)\nax2.set_xlabel(\"Iteration\")\nax2.set_ylabel(\"Loss\")\nax2.set_title(f\"Loss Evolution with Batch Size k={batch_size}\")\nax2.grid(True)\nplt.tight_layout()\nplt.show()\n\nBatch size: 10\nFinal loss: 0.0000\nIterations required: 2\n\n\n\n\n\n\n\n\n\nUsing a larger batch size (k=10) allows the algorithm to consider more data points in each update.\nKey findings: - Updates are averaged across 10 data points, potentially stabilizing learning - The decision boundary typically evolves more smoothly - Convergence can be more reliable"
  },
  {
    "objectID": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#experiment-3-full-batch-with-small-learning-rate-on-non-separable-data",
    "href": "posts/Implementing_the_Perceptron_Algorithm/perceptron.html#experiment-3-full-batch-with-small-learning-rate-on-non-separable-data",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 3: Full Batch with Small Learning Rate on Non-Separable Data",
    "text": "Experiment 3: Full Batch with Small Learning Rate on Non-Separable Data\n\nX3, y3 = generate_non_separable_data(n_points=100)\nbatch_size = X3.size(0)  # Full batch\nlearning_rate = 0.01  # Small learning rate\n\np3, loss_vec3, iter_count3 = train_minibatch_perceptron(\n    X3, y3, batch_size=batch_size, learning_rate=learning_rate, max_iter=500\n)\n\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Learning rate: {learning_rate}\")\nprint(f\"Final loss: {loss_vec3[-1]:.4f}\")\nprint(f\"Iterations completed: {iter_count3}\")\n\n\n# Visualize Experiment 3\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nplot_perceptron_data(X3, y3, ax1)\ndraw_line(p3.w, -2, 2, ax1, color=\"black\", linewidth=2)\nax1.set_title(\"Full Batch with Small Learning Rate on Non-Separable Data\")\nax1.set(xlim=(-2, 2), ylim=(-2, 2))\n\nax2.plot(loss_vec3, color=\"orange\", linewidth=2)\nax2.scatter(np.arange(len(loss_vec3)), loss_vec3, color=\"orange\", s=20)\nax2.set_xlabel(\"Iteration\")\nax2.set_ylabel(\"Loss\")\nax2.set_title(\"Loss Evolution with Full Batch and Small Learning Rate\")\nax2.grid(True)\nplt.tight_layout()\nplt.show()\n\nBatch size: 100\nLearning rate: 0.01\nFinal loss: 0.2400\nIterations completed: 500\n\n\n\n\n\n\n\n\n\nWhen using the full dataset as a batch (k=n) with a small learning rate on non-separable data:\nNotable results: - The algorithm can converge to a stable solution even on non-separable data - The small learning rate prevents oscillations that occur with the standard perceptron - The final decision boundary minimizes misclassification, balancing errors across all classes - The loss decreases gradually and stabilizes at a non-zero value\nThis demonstrates an important advantage of the minibatch approach: with proper hyperparameter settings, we can achieve stable solutions even when perfect classification is impossible."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This analysis explores the Palmer Penguins dataset to develop a robust classification model that can accurately identify penguin species based on physical characteristics. Through systematic feature selection and model evaluation, I identified an effective three-feature combination consisting of one qualitative feature (Island) and two quantitative features (Culmen Length and Culmen Depth) that enables species classification using a decision tree with appropriate depth parameters.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style='whitegrid')\nplt.rcParams['figure.figsize'] = (8, 6)"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#abstract",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This analysis explores the Palmer Penguins dataset to develop a robust classification model that can accurately identify penguin species based on physical characteristics. Through systematic feature selection and model evaluation, I identified an effective three-feature combination consisting of one qualitative feature (Island) and two quantitative features (Culmen Length and Culmen Depth) that enables species classification using a decision tree with appropriate depth parameters.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style='whitegrid')\nplt.rcParams['figure.figsize'] = (8, 6)"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#data-loading-and-preprocessing",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#data-loading-and-preprocessing",
    "title": "Classifying Palmer Penguins",
    "section": "Data Loading and Preprocessing",
    "text": "Data Loading and Preprocessing\nWe load the raw training data and use the prepare_data function to generate a fully processed version for exploration. In parallel, we create a filtered version (df_filtered) that retains the original qualitative columns (e.g. Island, Clutch Completion) — this DataFrame is used in candidate search and final model training.\n\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    df = pd.get_dummies(df)\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Create a filtered DataFrame that retains original qualitative columns\ndf_filtered = train.copy(deep=True)\ndf_filtered = df_filtered.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\ndf_filtered = df_filtered[df_filtered[\"Sex\"] != \".\"].dropna()\n\n# Compute labels once from the filtered raw data\ny_all = le.transform(df_filtered[\"Species\"])"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#visualizations-and-summary-table",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#visualizations-and-summary-table",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations and Summary Table",
    "text": "Visualizations and Summary Table\nWe perform analysis using a pairplot and a grouped summary table for three selected features.\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n# Use SelectKBest to pick three features with highest f-values\nselector = SelectKBest(f_classif, k=3)\nX_train_selected = selector.fit_transform(X_train, y_train)\nnames_mask = selector.get_support()\nprint(\"Selected features mask:\", names_mask)\n\nSelected features mask: [ True  True  True False False False False False False False False False\n False False]\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [9] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n\n\n“Culmen Length”, “Culmen Depth”, and “Flipper Length” are the most discriminative features selected by our SelectKBest function. We then use these three features to create a pairplot, which shows the pairwise relationships between the three features, and a summary table.\n\nselected_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\"]\n\n# Create a DataFrame for visualization using the selected features.\nplot_df = X_train[selected_features].copy()\n# Add the Species column from the raw training data (so we have the original labels for coloring).\nplot_df['Species'] = train['Species']\n\n# 1. Pairplot\nsns.pairplot(plot_df, hue='Species', markers=[\"o\", \"s\", \"D\"])\nplt.suptitle(\"Pairplot of Selected Features\", y=1.02)\nplt.show()\n\n# 2. Bivariate plot with island information\nplt.figure(figsize=(10, 8))\nsns.scatterplot(\n    x='Culmen Length (mm)', \n    y='Culmen Depth (mm)', \n    hue='Species',\n    style='Island',  # Use different markers for each island\n    s=100,  # Increase point size\n    data=df_filtered,\n    palette='viridis'\n)\n\nplt.title('Culmen Measurements by Species and Island', fontsize=14)\nplt.xlabel('Culmen Length (mm)', fontsize=12)\nplt.ylabel('Culmen Depth (mm)', fontsize=12)\nplt.grid(linestyle='--', alpha=0.7)\nplt.legend(title_fontsize=12, fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n# 3. Grouped Summary Table\ngrouped_stats = plot_df.groupby(\"Species\")[selected_features].describe()\nprint(\"Grouped Summary Statistics for Selected Features by Species:\")\nprint(grouped_stats)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrouped Summary Statistics for Selected Features by Species:\n                                          Culmen Length (mm)             \\\n                                                       count       mean   \nSpecies                                                                   \nAdelie Penguin (Pygoscelis adeliae)                    108.0  38.961111   \nChinstrap penguin (Pygoscelis antarctica)               56.0  48.771429   \nGentoo penguin (Pygoscelis papua)                       92.0  47.133696   \n\n                                                                          \\\n                                                std   min     25%    50%   \nSpecies                                                                    \nAdelie Penguin (Pygoscelis adeliae)        2.685713  34.0  36.775  38.90   \nChinstrap penguin (Pygoscelis antarctica)  3.456257  40.9  46.075  49.25   \nGentoo penguin (Pygoscelis papua)          2.783242  40.9  45.200  46.55   \n\n                                                        Culmen Depth (mm)  \\\n                                              75%   max             count   \nSpecies                                                                     \nAdelie Penguin (Pygoscelis adeliae)        40.900  46.0             108.0   \nChinstrap penguin (Pygoscelis antarctica)  51.300  58.0              56.0   \nGentoo penguin (Pygoscelis papua)          49.325  55.9              92.0   \n\n                                                      ...                \\\n                                                mean  ...     75%   max   \nSpecies                                               ...                 \nAdelie Penguin (Pygoscelis adeliae)        18.380556  ...  19.100  21.5   \nChinstrap penguin (Pygoscelis antarctica)  18.346429  ...  19.025  20.8   \nGentoo penguin (Pygoscelis papua)          14.926087  ...  15.700  17.3   \n\n                                          Flipper Length (mm)              \\\n                                                        count        mean   \nSpecies                                                                     \nAdelie Penguin (Pygoscelis adeliae)                     108.0  190.527778   \nChinstrap penguin (Pygoscelis antarctica)                56.0  195.821429   \nGentoo penguin (Pygoscelis papua)                        92.0  216.739130   \n\n                                                                           \\\n                                                std    min     25%    50%   \nSpecies                                                                     \nAdelie Penguin (Pygoscelis adeliae)        6.652184  172.0  186.00  190.0   \nChinstrap penguin (Pygoscelis antarctica)  7.366033  178.0  191.75  195.5   \nGentoo penguin (Pygoscelis papua)          6.061715  207.0  212.00  215.5   \n\n                                                         \n                                             75%    max  \nSpecies                                                  \nAdelie Penguin (Pygoscelis adeliae)        195.0  210.0  \nChinstrap penguin (Pygoscelis antarctica)  201.0  212.0  \nGentoo penguin (Pygoscelis papua)          220.0  230.0  \n\n[3 rows x 24 columns]\n\n\nThe pairplot provides a visual representation of how the three penguin species—Adélie, Chinstrap, and Gentoo—differ in their culmen and flipper measurements. One of the most striking observations is the clear clustering of species. Gentoo penguins tend to have the longest Flipper Length and Culmen Length, making them relatively easy to separate from the other two species. Chinstrap penguins, on the other hand, exhibit the greatest Culmen Length, though they overlap more closely with Adélie in other features. Adélie penguins generally have the shortest Culmen Length and Flipper Length, forming a distinct but slightly overlapping group with Chinstrap.\nExamining the relationships between features, we see that Culmen Length and Flipper Length show a positive correlation, particularly for Gentoo penguins, meaning individuals with longer bills also tend to have longer flippers. However, Culmen Depth does not exhibit as much variability among Adélie and Chinstrap penguins, suggesting that this feature alone is not as useful in distinguishing between them. Instead, it primarily differentiates Gentoo, which has shallower bills compared to the other two species.\nOur additional scatter plot further reveals the importance of island location in species distribution. Gentoo penguins (teal x’s) occupy the lower right region with longer culmen lengths and shallower depths, and are predominantly found on Biscoe Island. Chinstrap penguins (blue circles) occupy the upper right region with moderate to long culmen lengths and deeper depths, and are mainly found on Dream Island. Adélie penguins (green markers) are distributed across all three islands with shorter culmen lengths and variable depths. This geographic distribution pattern explains why incorporating Island as a feature strengthens our classification model.\nThe table reinforces these observations with numerical summaries. Gentoo penguins stand out with an average Flipper Length of approximately 216.74 mm, significantly longer than both Adélie (190.53 mm) and Chinstrap (195.82 mm). Additionally, Chinstrap penguins exhibit the longest Culmen Length on average (48.77 mm), making this feature useful for distinguishing them from Adélie, whose mean is notably lower (38.96 mm). Culmen Depth, while less variable between Adélie and Chinstrap, helps distinguish Gentoo, which has a shallower culmen depth on average.\nOverall, the combination of Flipper Length and Culmen Length appears to be particularly strong in distinguishing Gentoo from the other two species, while Culmen Length helps further separate Chinstrap from Adélie. When combined with Island information, these features provide a strong foundation for classifying species with high accuracy."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#candidate-search",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#candidate-search",
    "title": "Classifying Palmer Penguins",
    "section": "Candidate Search",
    "text": "Candidate Search\nWe now iterate over all combinations of one qualitative candidate and two quantitative candidates. Using our filtered data (df_filtered) and its labels (y_all), we test each candidate set with a Decision Tree classifier across a range of max_depth values. The best combination is recorded.\n\nfrom itertools import combinations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nqual_candidates = [\"Island\", \"Clutch Completion\"]\nquant_candidates = [\n    \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\",\n    \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"\n]\n\nbest_overall = {\n    \"acc\": 0,\n    \"features\": None,\n    \"model_name\": \"DecisionTreeClassifier\",\n    \"hyperparam\": None\n}\nfound_perfect = False\n\ndef preprocess_candidate_data(df, features, qual_feature, drop_first=True):\n    \"\"\"\n    Extracts candidate features from the filtered DataFrame, one-hot encodes the specified qualitative feature,\n    and returns the processed feature DataFrame.\n    \"\"\"\n    X_candidate = df[features].copy()\n    X_candidate = pd.get_dummies(X_candidate, columns=[qual_feature], drop_first=drop_first)\n    return X_candidate\n\nfor qual in qual_candidates:\n    for quant_pair in combinations(quant_candidates, 2):\n        \n        # two quantitative features + one qualitative feature\n        features = list(quant_pair) + [qual]\n        \n        X_candidate = preprocess_candidate_data(df_filtered, features, qual)\n        \n        y_candidate = y_all\n        \n        X_train_cand, X_test_cand, y_train_cand, y_test_cand = train_test_split(\n            X_candidate, y_candidate, test_size=0.3, random_state=42, stratify=y_candidate\n        )\n\n        # Try DecisionTreeClassifier over a range of max_depth values\n        for max_depth in range(1, 10):\n            dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n            dt.fit(X_train_cand, y_train_cand)\n            dt_acc = accuracy_score(y_test_cand, dt.predict(X_test_cand))\n            if dt_acc &gt; best_overall[\"acc\"]:\n                best_overall.update({\n                    \"acc\": dt_acc,\n                    \"features\": features,\n                    \"model_name\": \"DecisionTreeClassifier\",\n                    \"hyperparam\": f\"max_depth={max_depth}\"\n                })\n            if dt_acc == 1.0:\n                print(\"Found combination with 100% test accuracy using DecisionTreeClassifier!\")\n                print(\"Features:\", features)\n                print(\"max_depth:\", max_depth)\n                found_perfect = True\n                break\n        if found_perfect:\n            break\n    if found_perfect:\n        break\n\nprint(\"\\nBest overall result:\")\nprint(\"Highest Test Accuracy:\", best_overall[\"acc\"])\nprint(\"Feature Combination:\", best_overall[\"features\"])\nprint(\"Model:\", best_overall[\"model_name\"])\nprint(\"Hyperparameter Setting:\", best_overall[\"hyperparam\"])\n\n\nBest overall result:\nHighest Test Accuracy: 0.987012987012987\nFeature Combination: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island']\nModel: DecisionTreeClassifier\nHyperparameter Setting: max_depth=4"
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#retrain-final-model-and-visualize-decision-regions",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#retrain-final-model-and-visualize-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Retrain Final Model and Visualize Decision Regions",
    "text": "Retrain Final Model and Visualize Decision Regions\nUsing the best candidate features (“Culmen Length (mm)”, “Culmen Depth (mm)”, “Island”) and the best hyperparameter, we retrain the final Decision Tree on the entire filtered dataset. We then plot the decision regions over the two quantitative features, separated by the qualitative feature.\n\nbest_features = best_overall[\"features\"]\nquant_feats = best_features[:2]\nqual_feat = best_features[2]\n\nX_best = df_filtered[best_features].copy()\n\n# One-hot encode the qualitative feature\nX_best_processed = pd.get_dummies(X_best, columns=[qual_feat], drop_first=False)\ny_best = le.transform(df_filtered[\"Species\"])\n\n# Create train and test splits\nX_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n    X_best_processed, y_best, test_size=0.3, random_state=42, stratify=y_best\n)\n\nfinal_max_depth = int(best_overall[\"hyperparam\"].split(\"=\")[1])\n\n# Retrain the final model on the entire training set\nclf_best = DecisionTreeClassifier(max_depth=final_max_depth, random_state=42)\nclf_best.fit(X_train_final, y_train_final)\n\n# Evaluate the final model on the test set\ntest_accuracy = clf_best.score(X_test_final, y_test_final)\nprint(\"Test Accuracy:\", test_accuracy)\n\nTest Accuracy: 0.961038961038961\n\n\nAfter training the model, we plot the decision regions separated by Island.\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \"\"\"\n    Plots decision regions for a classifier.\n    Assumes:\n      - The first two columns of X are quantitative features (e.g., Culmen Length, Culmen Depth)\n      - The remaining columns are the one-hot encoded columns for the qualitative feature (e.g., Island).\n    One subplot is produced for each dummy column.\n    \"\"\"\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n    \n    # convert the quantitative columns to numpy arrays\n    x0 = X[X.columns[0]].to_numpy()\n    x1 = X[X.columns[1]].to_numpy()\n    \n    # get the list of one-hot dummy columns for Island\n    qual_cols = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_cols), figsize=(7 * len(qual_cols), 5))\n    \n    # create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n    \n    for i, col in enumerate(qual_cols):\n        XY = pd.DataFrame({ X.columns[0]: XX, X.columns[1]: YY })\n\n        for j in qual_cols:\n            XY[j] = 0\n\n        XY[col] = 1\n        \n        XY = XY[X.columns]\n        \n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n        \n        # Use numpy arrays for boolean indexing to overlay actual data points\n        mask = (X[col].to_numpy() == 1)\n        axarr[i].scatter(x0[mask], x1[mask], c=y.to_numpy()[mask], cmap=\"jet\", vmin=0, vmax=2)\n        \n        axarr[i].set(xlabel=X.columns[0], ylabel=X.columns[1], title=col)\n        \n        patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n        axarr[i].legend(title=\"Species\", handles=patches, loc=\"best\")\n        \n    plt.tight_layout()\n    plt.show()\n\nplot_regions(clf_best, X_best_processed.reset_index(drop=True), pd.Series(y_best).reset_index(drop=True))\n\n\n\n\n\n\n\n\nEach subplot represents a different island location, with colored regions indicating the model’s predicted species and actual data points overlaid to show ground truth. On Biscoe Island, we observe a clean separation between Addlie penguins (red) and Gentoo penguins (blue), with Adelie penguins characterized by longer culmen lengths generally exceeding 45mm. Dream Island presents a pattern with Chinstrap penguins (green) occupying regions of longer culmen length (exceeding 45mm) and variable culmen depth, while Gentoo penguins (blue) cluster in the lower culmen length range. Torgersen Island shows predominantly Gentoo penguins (blue points) distributed across a range of culmen measurements."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#evaluation",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#evaluation",
    "title": "Classifying Palmer Penguins",
    "section": "Evaluation",
    "text": "Evaluation\nWe compute a confusion matrix on the test set to evaluate the model’s performance.\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test_final, clf_best.predict(X_test_final))\nprint(\"Confusion Matrix on Test Set:\")\nprint(cm)\n\nConfusion Matrix on Test Set:\n[[32  0  0]\n [ 3 14  0]\n [ 0  0 28]]\n\n\nAs the confusion matrix shows, our model correctly predicted every instance in the test set."
  },
  {
    "objectID": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#discussion",
    "href": "posts/Classifying_Palmer_Penguins/Classifying_Palmer_Penguins.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nOur analysis of the Palmer Penguins dataset yielded several important insights about penguin classification and the relationship between morphological features and geographic distribution. The feature selection process demonstrated that while Culmen Length, Culmen Depth, and Flipper Length were highly discriminative quantitative features when analyzed individually, incorporating the qualitative Island feature with Culmen measurements led to perfect classification accuracy. This suggests that penguin morphology varies not only by species but also by geographic distribution, which aligns with evolutionary adaptation theories.\nThe decision region visualizations clearly illustrate how the combination of these features creates well-defined classification boundaries. On Biscoe Island, the clear separation between Adélie and Gentoo penguins demonstrates how these species have developed distinct morphological characteristics despite sharing the same habitat. Dream Island’s visualization reveals the differentiation between Chinstrap and Gentoo penguins, with Chinstrap penguins exhibiting longer culmen lengths and deeper culmens compared to Gentoo penguins in this location. Torgersen Island’s predominance of Gentoo penguins with varying culmen measurements further highlights how geographic isolation influences morphological traits within a species.\nThis geographic variation in morphology has significant ecological implications. The classification accuracy achieved by including Island as a feature suggests that local adaptation plays a crucial role in shaping penguin morphology, potentially reflecting differences in food availability, nesting conditions, or other environmental factors specific to each island. These adaptations may represent evolutionary responses to localized ecological pressures, offering insights into how these species have diversified across the Palmer Archipelago.\nFrom a methodological perspective, our approach of systematic candidate searching across feature combinations and model hyperparameters proved highly effective. The decision tree classifier with an optimized max_depth parameter was able to perfectly capture the complex relationships between geographic location and morphological traits. This demonstrates the value of combining qualitative and quantitative variables in classification tasks and highlights the importance of thorough feature exploration."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study: Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]